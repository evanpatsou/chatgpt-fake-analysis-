{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dede1d3",
   "metadata": {},
   "source": [
    "# SI Opportunity Scoring — Train/Validation Evaluation\n",
    "**Fixed rule → Data-driven rule → ML**  \n",
    "**Last updated:** 2026-02-24\n",
    "\n",
    "## Target (proxy) definition\n",
    "We derive **`si_offering`** from `OFFERING_NAME`:\n",
    "- `si_offering_row = 1` if OFFERING_NAME contains the token **SI** (case-insensitive)\n",
    "- `si_offering (per ID) = max(si_offering_row)` across rows for that client\n",
    "\n",
    "### Objective\n",
    "Rank **IDs with `si_offering = 0`** (not currently in an SI offering) by **likelihood of SI interest** using preference signals.\n",
    "\n",
    "> Important: `si_offering` is a *proxy* label (membership ≠ true interest).  \n",
    "> This notebook evaluates models **against the proxy** and ends with a pilot plan for true labels.\n",
    "\n",
    "**Update:** raw rows are shuffled for robustness; ML benchmark is Calibrated Logistic Regression only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538786a",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76779193",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Load data (fallback demo)\n",
    "\n",
    "Expected columns:\n",
    "- `ID`, `IO_TYPE`, `LIFE_CYCLE`, `OFFERING_NAME`\n",
    "- `SI_CONSIDERATION_CD`, `SFDR_PREF`, `SFDR_ACTUAL`, `PAI_PREF`, `MIFID`\n",
    "- ESG topics: `GHG`, `Biodiversity`, `Water`, `Waste`, `Social`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5986a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data.csv\")  # <-- change to your real file path\n",
    "\n",
    "def make_synthetic_data(n=8000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": rng.integers(1, n//2 + 1, size=n),  # multiple rows per ID\n",
    "        \"IO_TYPE\": rng.choice([\"normal\", \"zombie\"], size=n, p=[0.97, 0.03]),\n",
    "        \"LIFE_CYCLE\": rng.choice([\"open\", \"closed\"], size=n, p=[0.9, 0.1]),\n",
    "\n",
    "        \"SI_CONSIDERATION_CD\": rng.choice([\"S1\",\"S2\",\"S3\", None], size=n, p=[0.35,0.35,0.2,0.1]),\n",
    "        \"SFDR_PREF\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.4,0.35,0.2,0.05]),\n",
    "        \"SFDR_ACTUAL\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.45,0.35,0.15,0.05]),\n",
    "        \"PAI_PREF\": rng.choice([\"PAI Selected\", None], size=n, p=[0.3,0.7]),\n",
    "        \"MIFID\": rng.choice([\"Yes\",\"No\", None], size=n, p=[0.55,0.4,0.05]),\n",
    "\n",
    "        \"GHG\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.25,0.65,0.05,0.05]),\n",
    "        \"Biodiversity\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.2,0.7,0.05,0.05]),\n",
    "        \"Water\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.22,0.68,0.05,0.05]),\n",
    "        \"Waste\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.18,0.72,0.05,0.05]),\n",
    "        \"Social\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.28,0.62,0.05,0.05]),\n",
    "    })\n",
    "\n",
    "    df[\"OFFERING_NAME\"] = rng.choice(\n",
    "        [\"Core\", \"Standard\", \"ESG Plus\", \"SI Focus\", \"Core SI\", \"Income\", \"SI Sustainable\"],\n",
    "        size=n, p=[0.25,0.25,0.15,0.15,0.08,0.07,0.05]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded: {DATA_PATH}  shape={df_raw.shape}\")\n",
    "else:\n",
    "    df_raw = make_synthetic_data()\n",
    "    print(\"DATA_PATH not found; using synthetic demo dataset.\")\n",
    "    print(f\"shape={df_raw.shape}\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bab283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible shuffle of raw rows (helps avoid ordering artifacts)\n",
    "df_raw = df_raw.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75777f63",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Derive `si_offering` from name and aggregate to ID-level\n",
    "\n",
    "We must operate at **ID-level** to avoid recommending the same client multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb01dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Row-level SI membership flag derived from OFFERING_NAME\n",
    "df[\"si_offering_row\"] = df[\"OFFERING_NAME\"].astype(str).str.contains(r\"\\bSI\\b\", case=False, na=False).astype(int)\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    if len(s2) == 0:\n",
    "        return np.nan\n",
    "    m = s2.mode()\n",
    "    if len(m) > 0:\n",
    "        return m.iloc[0]\n",
    "    return s2.iloc[0]\n",
    "\n",
    "agg_dict = {\n",
    "    \"IO_TYPE\": mode_or_first,\n",
    "    \"LIFE_CYCLE\": mode_or_first,\n",
    "    \"SI_CONSIDERATION_CD\": mode_or_first,\n",
    "    \"SFDR_PREF\": mode_or_first,\n",
    "    \"SFDR_ACTUAL\": mode_or_first,\n",
    "    \"PAI_PREF\": mode_or_first,\n",
    "    \"MIFID\": mode_or_first,\n",
    "    \"GHG\": mode_or_first,\n",
    "    \"Biodiversity\": mode_or_first,\n",
    "    \"Water\": mode_or_first,\n",
    "    \"Waste\": mode_or_first,\n",
    "    \"Social\": mode_or_first,\n",
    "    \"si_offering_row\": \"max\",\n",
    "}\n",
    "\n",
    "df_id = df.groupby(\"ID\", as_index=False).agg(agg_dict).rename(columns={\"si_offering_row\":\"si_offering\"})\n",
    "\n",
    "print(\"Row-level rows:\", len(df))\n",
    "print(\"ID-level rows :\", len(df_id))\n",
    "print(\"si_offering rate (ID-level):\", df_id[\"si_offering\"].mean().round(4))\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3dd2b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Cleaning rules & impact\n",
    "\n",
    "- Remove `IO_TYPE='zombie'`\n",
    "- Keep `LIFE_CYCLE='open'`\n",
    "- Standardize missing\n",
    "- Convert topics/MiFID/PAI to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad299111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_to_1(x):\n",
    "    if pd.isna(x): return 0\n",
    "    x = str(x).strip()\n",
    "    if x == \"--\": return 0\n",
    "    return 1 if x.lower() == \"yes\" else 0\n",
    "\n",
    "def clean_id_level(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df[df[\"IO_TYPE\"].fillna(\"\").str.lower() != \"zombie\"]\n",
    "    df = df[df[\"LIFE_CYCLE\"].fillna(\"\").str.lower() == \"open\"]\n",
    "\n",
    "    for c in [\"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\"]:\n",
    "        df[c] = df[c].astype(\"object\").where(df[c].notna(), \"nan\")\n",
    "\n",
    "    for c in [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]:\n",
    "        df[c] = df[c].apply(yes_to_1).astype(int)\n",
    "\n",
    "    df[\"MIFID\"] = df[\"MIFID\"].apply(yes_to_1).astype(int)\n",
    "    df[\"PAI_PREF\"] = (df[\"PAI_PREF\"].astype(str).str.lower() == \"pai selected\").astype(int)\n",
    "\n",
    "    df[\"si_offering\"] = df[\"si_offering\"].astype(int)\n",
    "    return df\n",
    "\n",
    "df_clean = clean_id_level(df_id)\n",
    "\n",
    "impact = pd.DataFrame({\n",
    "    \"stage\": [\"before\", \"after\"],\n",
    "    \"rows\": [len(df_id), len(df_clean)],\n",
    "    \"si_offering_rate\": [df_id[\"si_offering\"].mean(), df_clean[\"si_offering\"].mean()]\n",
    "})\n",
    "impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea3645",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Feature engineering (including sfdr_gap)\n",
    "\n",
    "We explicitly avoid any feature derived from OFFERING_NAME beyond the label `si_offering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SI = {\"S1\":1, \"S2\":2, \"S3\":3, \"nan\": np.nan}\n",
    "MAP_SFDR = {\"F1\":1, \"F2\":2, \"F3\":3, \"nan\": np.nan}\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"SI_CONSIDERATION_num\"] = df[\"SI_CONSIDERATION_CD\"].map(MAP_SI).fillna(1).astype(int)\n",
    "    df[\"SFDR_PREF_num\"] = df[\"SFDR_PREF\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "    df[\"SFDR_ACTUAL_num\"] = df[\"SFDR_ACTUAL\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "\n",
    "    df[\"sfdr_gap\"] = np.clip(df[\"SFDR_PREF_num\"] - df[\"SFDR_ACTUAL_num\"], -2, 2)\n",
    "    df[\"sfdr_opp\"] = np.maximum(df[\"sfdr_gap\"], 0)  # 0..2\n",
    "\n",
    "    topic_cols = [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]\n",
    "    df[\"esg_topics_yes_cnt\"] = df[topic_cols].sum(axis=1)\n",
    "    df[\"esg_topics_yes_share\"] = df[\"esg_topics_yes_cnt\"] / len(topic_cols)\n",
    "\n",
    "    # normalized (0..1)\n",
    "    df[\"si_norm\"] = np.clip((df[\"SI_CONSIDERATION_num\"] - 1)/2, 0, 1)\n",
    "    df[\"sfdr_norm\"] = np.clip(df[\"sfdr_opp\"]/2, 0, 1)\n",
    "    df[\"topics_norm\"] = np.clip(df[\"esg_topics_yes_share\"], 0, 1)\n",
    "    df[\"topics_if_pai\"] = df[\"topics_norm\"] * df[\"PAI_PREF\"]  # topics only matter if PAI=1\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = engineer_features(df_clean)\n",
    "df_feat[[\"ID\",\"si_offering\",\"MIFID\",\"SI_CONSIDERATION_num\",\"sfdr_gap\",\"PAI_PREF\",\"esg_topics_yes_cnt\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ed9ff",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Fixed-weight rule score (branching)\n",
    "\n",
    "**Logic:**\n",
    "- If `MIFID=0`: SI-only (capped)\n",
    "- If `MIFID=1`: 70% SFDR opportunity + 30% PAI block (topics only if PAI=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b181665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RuleConfig:\n",
    "    # Branch A (MIFID=0): capped SI mapping to avoid overconfidence\n",
    "    si_score_s1: float = 20\n",
    "    si_score_s2: float = 50\n",
    "    si_score_s3: float = 80\n",
    "    # Branch B (MIFID=1)\n",
    "    w_sfdr: float = 0.70\n",
    "    w_pai_block: float = 0.30\n",
    "\n",
    "cfg = RuleConfig()\n",
    "\n",
    "def score_fixed_rule(df: pd.DataFrame, cfg: RuleConfig) -> pd.Series:\n",
    "    si_score = df[\"SI_CONSIDERATION_num\"].map({1: cfg.si_score_s1, 2: cfg.si_score_s2, 3: cfg.si_score_s3}).astype(float)\n",
    "    pai_block = np.where(df[\"PAI_PREF\"] == 1, 0.5 + 0.5*df[\"topics_norm\"], 0.0)  # 0..1\n",
    "    score_B = 100 * (cfg.w_sfdr * df[\"sfdr_norm\"] + cfg.w_pai_block * pai_block)\n",
    "    score = np.where(df[\"MIFID\"]==1, score_B, si_score)\n",
    "    return pd.Series(np.clip(score, 0, 100), index=df.index)\n",
    "\n",
    "df_feat[\"score_fixed\"] = score_fixed_rule(df_feat, cfg)\n",
    "df_feat[[\"score_fixed\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cd647",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Train/Validation split (ID-level)\n",
    "\n",
    "We use a stratified split on the proxy label `si_offering`.\n",
    "Rule scores do not train, but we still evaluate them on the held-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edddfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"si_norm\",\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\",\"esg_topics_yes_cnt\",\"sfdr_gap\",\"MIFID\",\"SI_CONSIDERATION_num\"]\n",
    "\n",
    "X = df_feat[FEATURES].copy()\n",
    "y = df_feat[\"si_offering\"].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "idx_train = X_train.index\n",
    "idx_val = X_val.index\n",
    "\n",
    "print(\"Train size:\", len(idx_train), \"Val size:\", len(idx_val))\n",
    "print(\"Train si_offering rate:\", y_train.mean().round(4), \"Val si_offering rate:\", y_val.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71511f86",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) Evaluation helpers (AUC/AP/Brier + lift-by-decile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abe973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scores(y_true, p, label):\n",
    "    out = {\n",
    "        \"model\": label,\n",
    "        \"auc\": roc_auc_score(y_true, p),\n",
    "        \"avg_precision\": average_precision_score(y_true, p),\n",
    "        \"brier\": brier_score_loss(y_true, p)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def lift_table(y_true, p, n_bins=10):\n",
    "    tmp = pd.DataFrame({\"y\": y_true, \"p\": p})\n",
    "    tmp[\"bin\"] = pd.qcut(tmp[\"p\"], n_bins, labels=False, duplicates=\"drop\") + 1\n",
    "    tab = tmp.groupby(\"bin\")[\"y\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"si_rate\"})\n",
    "    return tab\n",
    "\n",
    "def plot_lift(tab, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(tab.index, tab[\"si_rate\"].values, marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Decile (1=lowest score, 10=highest)\")\n",
    "    plt.ylabel(\"si_offering rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13638592",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Fixed-weight rule — validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fixed_val = (df_feat.loc[idx_val, \"score_fixed\"].values / 100.0)\n",
    "fixed_metrics = eval_scores(y_val.values, p_fixed_val, \"Fixed-weight rule\")\n",
    "fixed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_fixed = lift_table(y_val.values, p_fixed_val, n_bins=10)\n",
    "lift_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1691a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lift(lift_fixed, \"Lift (proxy): Fixed-weight rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f167c2",
   "metadata": {},
   "source": [
    "---\n",
    "## 9) Data-driven rule (learn weights on train, apply on validation)\n",
    "\n",
    "We learn weights using a regularized logistic regression on train, then convert coefficients to weights summing to 100.\n",
    "This keeps the scoring **rule-like** and stakeholder-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-driven rule features (keep aligned to your branching logic)\n",
    "DD_FEATURES = [\"si_norm\",\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\"]\n",
    "\n",
    "lr_dd = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "lr_dd.fit(X_train[DD_FEATURES], y_train)\n",
    "\n",
    "coef = pd.Series(lr_dd.coef_[0], index=DD_FEATURES).sort_values(key=np.abs, ascending=False)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33570442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_to_100_weights(coef_series):\n",
    "    pos = np.maximum(coef_series.values, 0)\n",
    "    if pos.sum() == 0:\n",
    "        pos = np.ones_like(pos)\n",
    "    w = 100 * pos / pos.sum()\n",
    "    return pd.Series(w, index=coef_series.index).sort_values(ascending=False)\n",
    "\n",
    "w_dd = coef_to_100_weights(coef)\n",
    "w_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-like score: weights sum to 100, features are 0..1, so score is 0..100\n",
    "df_feat[\"score_datadriven\"] = (\n",
    "    w_dd[\"si_norm\"] * df_feat[\"si_norm\"] +\n",
    "    w_dd[\"sfdr_norm\"] * df_feat[\"sfdr_norm\"] +\n",
    "    w_dd[\"PAI_PREF\"] * df_feat[\"PAI_PREF\"] +\n",
    "    w_dd[\"topics_if_pai\"] * df_feat[\"topics_if_pai\"]\n",
    ").clip(0,100)\n",
    "\n",
    "p_dd_val = df_feat.loc[idx_val, \"score_datadriven\"].values / 100.0\n",
    "dd_metrics = eval_scores(y_val.values, p_dd_val, \"Data-driven rule (LR weights)\")\n",
    "dd_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8020ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_dd = lift_table(y_val.values, p_dd_val, n_bins=10)\n",
    "plot_lift(lift_dd, \"Lift (proxy): Data-driven rule\")\n",
    "lift_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879203b",
   "metadata": {},
   "source": [
    "## 10) ML model (validation): Calibrated Logistic Regression\n",
    "\n",
    "We use **Calibrated Logistic Regression** as the ML benchmark because it is:\n",
    "- Strong and stable for tabular preference data\n",
    "- Interpretable (coefficients can be inspected)\n",
    "- Calibrated probabilities support thresholding and bucket definitions\n",
    "\n",
    "We fit on the **train** split and evaluate on the **validation** split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set for ML\n",
    "ML_FEATURES = [\"si_norm\",\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\",\"esg_topics_yes_cnt\",\"sfdr_gap\",\"MIFID\",\"SI_CONSIDERATION_num\"]\n",
    "\n",
    "Xtr = X_train[ML_FEATURES].copy()\n",
    "Xva = X_val[ML_FEATURES].copy()\n",
    "\n",
    "# Calibrated Logistic Regression (isotonic calibration)\n",
    "lr = LogisticRegression(max_iter=3000, class_weight=\"balanced\")\n",
    "cal_lr = CalibratedClassifierCV(lr, method=\"isotonic\", cv=5)\n",
    "cal_lr.fit(Xtr, y_train)\n",
    "\n",
    "p_lr = cal_lr.predict_proba(Xva)[:,1]\n",
    "\n",
    "results = [\n",
    "    fixed_metrics,\n",
    "    dd_metrics,\n",
    "    eval_scores(y_val.values, p_lr, \"ML: Calibrated Logistic Regression\"),\n",
    "]\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"auc\", ascending=False).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1db1b",
   "metadata": {},
   "source": [
    "## 11) Recommendation\n",
    "\n",
    "For this use case, the strongest and safest ML option is **Calibrated Logistic Regression**:\n",
    "- It performs well on tabular preference data\n",
    "- It stays explainable (coefficients can be reviewed)\n",
    "- Calibration provides probability-like outputs suitable for percentile buckets\n",
    "\n",
    "Operationally, you can deploy with **Fixed-weight rule** or **Data-driven rule** and treat ML as an enhancement if it improves validation lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fcf47",
   "metadata": {},
   "source": [
    "---\n",
    "## 12) Operational output: target IDs (si_offering=0) ranked by chosen score\n",
    "\n",
    "Choose a score for ranking:\n",
    "- `score_fixed` (fixed rule)\n",
    "- `score_datadriven` (data-driven rule)\n",
    "- `p_lr` (Calibrated Logistic Regression probability on validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3dc3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For production, you would refit the chosen model on the full dataset.\n",
    "# Here, we demonstrate ranking using rule scores (available for all rows).\n",
    "\n",
    "RANK_SCORE = \"score_datadriven\"  # change to \"score_fixed\" if you prefer\n",
    "\n",
    "df_out = df_feat.copy()\n",
    "df_out[\"score_percentile\"] = (df_out[RANK_SCORE].rank(pct=True) * 100).round(2)\n",
    "df_out[\"bucket_3\"] = pd.cut(df_out[\"score_percentile\"], bins=[-0.01, 50, 80, 100], labels=[\"Low\",\"Average\",\"High\"])\n",
    "\n",
    "targets = df_out[df_out[\"si_offering\"]==0].sort_values(RANK_SCORE, ascending=False)\n",
    "\n",
    "cols = [\"ID\", RANK_SCORE, \"score_percentile\", \"bucket_3\", \"MIFID\",\"SI_CONSIDERATION_num\",\"sfdr_gap\",\"PAI_PREF\",\"esg_topics_yes_cnt\"]\n",
    "targets[cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c4c27",
   "metadata": {},
   "source": [
    "---\n",
    "## 13) Pilot plan (create true labels)\n",
    "\n",
    "Because `si_offering` is a proxy, validate value with a pilot:\n",
    "- Treatment: High bucket (top 20% of `si_offering=0`)\n",
    "- Control: random sample from eligible pool (or next bucket)\n",
    "- Outcomes: response, meeting booked, adoption, pipeline created\n",
    "\n",
    "After pilot: retrain on true outcomes and re-evaluate Fixed vs Data-driven vs ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
