{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296cc54f",
   "metadata": {},
   "source": [
    "# SI Opportunity Scoring — Stakeholder Notebook (v7)\n",
    "**Three methods, one evaluation framework:**  \n",
    "1) **Fixed rule** (explicit business logic) →  \n",
    "2) **Weighted rule** (same logic, weights learned from data) →  \n",
    "3) **ML** (Calibrated Logistic Regression, controlled enhancement)\n",
    "\n",
    "**Purpose:** rank clients with **`si_offering = 0`** by likelihood of SI interest.\n",
    "\n",
    "---\n",
    "\n",
    "## How to read this notebook\n",
    "Each section follows the same decision-making template:\n",
    "\n",
    "1) **Question** — what are we trying to decide?  \n",
    "2) **Approach** — what logic do we apply?  \n",
    "3) **Evidence** — charts/tables we use to verify it works.  \n",
    "4) **Decision** — what we do next based on the evidence.\n",
    "\n",
    "This is intentionally designed so the **results lead to the next step**, and code only exists to generate those results.\n",
    "\n",
    "---\n",
    "\n",
    "## Proxy target (benchmark only)\n",
    "We create a proxy label `si_offering` from `OFFERING_NAME` containing the token **SI**.\n",
    "\n",
    "> This is *not* the final truth label (membership ≠ interest).  \n",
    "> We use it only to compare methods consistently until a pilot creates true outcomes (responses/adoption).\n",
    "\n",
    "---\n",
    "\n",
    "## Business logic (requested)\n",
    "- **If `MIFID = 0`:** use **only** `SI_CONSIDERATION` (S1=no, S2=some, S3=high).  \n",
    "- **If `MIFID = 1`:** use SFDR/PAI/Taxonomy **only when** `SI_CONSIDERATION` is **high (S3)**.  \n",
    "  Otherwise keep a conservative baseline.\n",
    "\n",
    "We implement that structure for Fixed + Weighted rules, and we respect the same structure in ML using interaction (“gate”) features.\n",
    "\n",
    "**Last updated:** 2026-02-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1157892",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "### What this section does\n",
    "- Imports the libraries used throughout (pandas/numpy for data handling, sklearn for evaluation and ML).\n",
    "- Sets display options for readable tables.\n",
    "\n",
    "### Decision checkpoint\n",
    "No decisions here—this is only to ensure the notebook runs reproducibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc725518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d264b8",
   "metadata": {},
   "source": [
    "## 1) Load raw data + shuffle rows\n",
    "\n",
    "### Question\n",
    "Do we have the raw fields we need, and is the dataset order influencing anything?\n",
    "\n",
    "### Approach\n",
    "- Load the raw dataset (or a synthetic demo if the file is not found).\n",
    "- **Shuffle rows** (`sample(frac=1, random_state=42)`) to remove accidental ordering artifacts.\n",
    "  - Many operational extracts are sorted by ID or date; shuffling makes splits and charts more robust.\n",
    "\n",
    "### Evidence\n",
    "We display the head of the dataframe to confirm columns and rough values.\n",
    "\n",
    "### Decision\n",
    "Proceed to cleaning/filtering once required raw columns are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb491a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data.csv\")  # <-- change to your real file path\n",
    "\n",
    "def make_synthetic_data(n=9000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return pd.DataFrame({\n",
    "        \"ID\": rng.integers(1, n//2 + 1, size=n),\n",
    "        \"IO_TYPE\": rng.choice([\"normal\", \"zombie\"], size=n, p=[0.97, 0.03]),\n",
    "        \"LIFE_CYCLE\": rng.choice([\"open\", \"closed\"], size=n, p=[0.9, 0.1]),\n",
    "        \"OFFERING_NAME\": rng.choice(\n",
    "            [\"Core\", \"Standard\", \"ESG Plus\", \"SI Focus\", \"Core SI\", \"Income\", \"SI Sustainable\", None],\n",
    "            size=n, p=[0.23,0.23,0.14,0.14,0.08,0.07,0.06,0.05]\n",
    "        ),\n",
    "        \"SI_CONSIDERATION_CD\": rng.choice([\"S1\",\"S2\",\"S3\", None], size=n, p=[0.35,0.35,0.2,0.1]),\n",
    "        \"SFDR_PREF\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.4,0.35,0.2,0.05]),\n",
    "        \"SFDR_ACTUAL\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.45,0.35,0.15,0.05]),\n",
    "        \"PAI_PREF\": rng.choice([\"PAI Selected\", None], size=n, p=[0.3,0.7]),\n",
    "        \"MIFID\": rng.choice([\"Yes\",\"No\", None], size=n, p=[0.55,0.4,0.05]),\n",
    "        \"TAXONOMYPREF\": rng.choice([\"A1\",\"A2\",\"A3\", None], size=n, p=[0.5,0.35,0.1,0.05]),\n",
    "        \"GHG\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.25,0.65,0.05,0.05]),\n",
    "        \"Biodiversity\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.2,0.7,0.05,0.05]),\n",
    "        \"Water\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.22,0.68,0.05,0.05]),\n",
    "        \"Waste\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.18,0.72,0.05,0.05]),\n",
    "        \"Social\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.28,0.62,0.05,0.05]),\n",
    "    })\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded: {DATA_PATH}  shape={df_raw.shape}\")\n",
    "else:\n",
    "    df_raw = make_synthetic_data()\n",
    "    print(\"DATA_PATH not found; using synthetic demo dataset.\")\n",
    "    print(f\"shape={df_raw.shape}\")\n",
    "\n",
    "df_raw = df_raw.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7496ec",
   "metadata": {},
   "source": [
    "## 2) Cleaning & filtering (ONLY)\n",
    "\n",
    "### Question\n",
    "Which rows are actionable, and what is the effect of removing non-actionable records?\n",
    "\n",
    "### Approach (data hygiene only)\n",
    "We **only**:\n",
    "- Trim whitespace in text fields\n",
    "- Convert placeholder missing values (`\"--\"`, empty string) to nulls\n",
    "- Filter out:\n",
    "  - `IO_TYPE = zombie`\n",
    "  - `LIFE_CYCLE != open`\n",
    "\n",
    "**Important:**  \n",
    "We do **not** map categories to numbers here (no Yes/No→1/0, no A1/A2/A3→1/2/3).  \n",
    "That separation makes the process easier to review and approve.\n",
    "\n",
    "### Evidence\n",
    "- A small summary table: rows before/after each filter\n",
    "- A bar chart showing the impact of filters\n",
    "- Missingness snapshot (still in raw form)\n",
    "\n",
    "### Decision checkpoint\n",
    "If cleaning removes too many rows (or shifts the proxy target rate heavily), revisit filters or upstream data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531d5cc",
   "metadata": {},
   "source": [
    "**How to interpret the outputs below**\n",
    "- The row-count chart should show a reasonable drop from removing zombies/closed lifecycle.\n",
    "- If the drop is extreme, we may be filtering too aggressively (or upstream data needs fixes).\n",
    "- Missingness helps decide which fields can safely influence scoring; high missingness fields should have lower weight (or require a missing-safe design)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab23aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_RAW = [\n",
    "    \"ID\",\"IO_TYPE\",\"LIFE_CYCLE\",\"OFFERING_NAME\",\n",
    "    \"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\",\"PAI_PREF\",\"MIFID\",\"TAXONOMYPREF\",\n",
    "    \"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"\n",
    "]\n",
    "missing_cols = [c for c in REQUIRED_RAW if c not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required raw columns: {missing_cols}\")\n",
    "\n",
    "def clean_filter_only(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # strip whitespace for object cols\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # standardize placeholder missing\n",
    "    df = df.replace({\"--\": np.nan, \"\": np.nan})\n",
    "\n",
    "    before = len(df)\n",
    "    df = df[df[\"IO_TYPE\"].fillna(\"\").str.lower() != \"zombie\"]\n",
    "    after_zombie = len(df)\n",
    "    df = df[df[\"LIFE_CYCLE\"].fillna(\"\").str.lower() == \"open\"]\n",
    "    after_open = len(df)\n",
    "\n",
    "    df.attrs[\"cleaning_summary\"] = {\n",
    "        \"before\": before,\n",
    "        \"after_remove_zombie\": after_zombie,\n",
    "        \"after_keep_open\": after_open,\n",
    "        \"removed_zombie\": before - after_zombie,\n",
    "        \"removed_closed\": after_zombie - after_open\n",
    "    }\n",
    "    return df\n",
    "\n",
    "df_clean_rows = clean_filter_only(df_raw)\n",
    "pd.DataFrame([df_clean_rows.attrs[\"cleaning_summary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charts: cleaning impact + missingness snapshot\n",
    "s = df_clean_rows.attrs[\"cleaning_summary\"]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar([\"Before\", \"After zombie\", \"After open\"], [s[\"before\"], s[\"after_remove_zombie\"], s[\"after_keep_open\"]])\n",
    "plt.title(\"Cleaning impact: rows remaining after filters\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "key_cols = [\"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\",\"TAXONOMYPREF\",\"MIFID\",\"PAI_PREF\",\n",
    "            \"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\",\"OFFERING_NAME\"]\n",
    "miss = df_clean_rows[key_cols].isna().mean().sort_values(ascending=False)\n",
    "display(miss.to_frame(\"missing_rate\").head(12))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(miss.index[:12], miss.values[:12])\n",
    "plt.title(\"Top missing rates after cleaning (raw values, before encoding)\")\n",
    "plt.ylabel(\"Missing rate\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59992045",
   "metadata": {},
   "source": [
    "## 3) Derive proxy label and aggregate to ID-level\n",
    "\n",
    "### Question\n",
    "What is the unit of action—rows or clients—and how do we create a benchmark label?\n",
    "\n",
    "### Approach\n",
    "1) Create `si_offering_row` from `OFFERING_NAME` containing token **SI**.\n",
    "2) Aggregate to a **single record per ID** (because we recommend clients, not rows).\n",
    "   - `si_offering = max(si_offering_row)` per ID.\n",
    "   - For preference fields, we use **mode (or first if tie)** for transparency.\n",
    "\n",
    "### Evidence\n",
    "- Size comparison: cleaned row-level vs ID-level\n",
    "- Proxy label prevalence (`si_offering` rate)\n",
    "\n",
    "### Decision checkpoint\n",
    "If the proxy label rate is extremely low/high, interpretation of lift metrics changes (AP becomes more important than AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e042ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean_rows.copy()\n",
    "\n",
    "# Row-level SI membership derived from OFFERING_NAME\n",
    "df[\"si_offering_row\"] = df[\"OFFERING_NAME\"].astype(str).str.contains(r\"\\bSI\\b\", case=False, na=False).astype(int)\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    if len(s2) == 0:\n",
    "        return np.nan\n",
    "    m = s2.mode()\n",
    "    return m.iloc[0] if len(m) else s2.iloc[0]\n",
    "\n",
    "agg_dict = {\n",
    "    \"IO_TYPE\": mode_or_first,\n",
    "    \"LIFE_CYCLE\": mode_or_first,\n",
    "    \"OFFERING_NAME\": mode_or_first,  # transparency only (not used as feature)\n",
    "    \"SI_CONSIDERATION_CD\": mode_or_first,\n",
    "    \"SFDR_PREF\": mode_or_first,\n",
    "    \"SFDR_ACTUAL\": mode_or_first,\n",
    "    \"PAI_PREF\": mode_or_first,\n",
    "    \"MIFID\": mode_or_first,\n",
    "    \"TAXONOMYPREF\": mode_or_first,\n",
    "    \"GHG\": mode_or_first,\n",
    "    \"Biodiversity\": mode_or_first,\n",
    "    \"Water\": mode_or_first,\n",
    "    \"Waste\": mode_or_first,\n",
    "    \"Social\": mode_or_first,\n",
    "    \"si_offering_row\": \"max\",\n",
    "}\n",
    "\n",
    "df_id = df.groupby(\"ID\", as_index=False).agg(agg_dict).rename(columns={\"si_offering_row\":\"si_offering\"})\n",
    "\n",
    "sizes = pd.DataFrame({\n",
    "    \"level\": [\"row-level (cleaned)\", \"ID-level\"],\n",
    "    \"rows\": [len(df), len(df_id)],\n",
    "    \"si_offering_rate\": [df[\"si_offering_row\"].mean(), df_id[\"si_offering\"].mean()]\n",
    "})\n",
    "display(sizes)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(sizes[\"level\"], sizes[\"rows\"])\n",
    "plt.title(\"Size: cleaned rows vs ID-level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(sizes[\"level\"], sizes[\"si_offering_rate\"])\n",
    "plt.title(\"Proxy label prevalence: si_offering rate\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b7602",
   "metadata": {},
   "source": [
    "## 4) Feature encoding & engineering (numbers happen here)\n",
    "\n",
    "### Question\n",
    "How do we convert preference fields into stable, explainable numeric signals?\n",
    "\n",
    "### Approach\n",
    "We transform raw fields into a small set of model-ready, interpretable signals:\n",
    "\n",
    "**Encodings**\n",
    "- `SI_CONSIDERATION_CD`: S1/S2/S3 → 1/2/3\n",
    "- `SFDR_PREF`, `SFDR_ACTUAL`: F1/F2/F3 → 1/2/3\n",
    "- `TAXONOMYPREF`: A1/A2/A3 → 1/2/3\n",
    "- Topics: Yes/No → 1/0\n",
    "- `MIFID`: Yes/No → 1/0\n",
    "- `PAI_PREF`: “PAI Selected” → 1 else 0\n",
    "\n",
    "**Engineered features**\n",
    "- `sfdr_gap = clip(SFDR_PREF - SFDR_ACTUAL, -2, 2)`\n",
    "- `sfdr_opp = max(sfdr_gap, 0)`  \n",
    "  Only positive gap is treated as opportunity (preference exceeds current state).\n",
    "- `pai_block` (0..1):  \n",
    "  0 if no PAI, else `0.5 + 0.5*topics_norm`  \n",
    "  (PAI selection plus intensity of ESG topic choices)\n",
    "- `tax_norm` scales A1/A2/A3 → 0..1\n",
    "- `si_is_s3` is a helper flag for the requested gate\n",
    "\n",
    "### Evidence\n",
    "We plot distributions (e.g., sfdr_gap and taxonomy) to confirm:\n",
    "- values are in expected ranges\n",
    "- signals are not mostly missing/zero\n",
    "\n",
    "### Decision checkpoint\n",
    "If a key signal is almost always missing/zero, it should not carry much weight (rule or ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SI = {\"S1\":1, \"S2\":2, \"S3\":3}\n",
    "MAP_SFDR = {\"F1\":1, \"F2\":2, \"F3\":3}\n",
    "MAP_TAX = {\"A1\":1, \"A2\":2, \"A3\":3}\n",
    "\n",
    "def yes_to_1(x):\n",
    "    return 1 if isinstance(x, str) and x.strip().lower() == \"yes\" else 0\n",
    "\n",
    "def encode_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Binary fields\n",
    "    for c in [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]:\n",
    "        df[c] = df[c].apply(yes_to_1).astype(int)\n",
    "\n",
    "    df[\"MIFID\"] = df[\"MIFID\"].apply(yes_to_1).astype(int)\n",
    "    df[\"PAI_PREF\"] = (df[\"PAI_PREF\"].astype(str).str.lower() == \"pai selected\").astype(int)\n",
    "\n",
    "    # Ordinals (conservative default to lowest tier)\n",
    "    df[\"SI_CONSIDERATION_num\"] = df[\"SI_CONSIDERATION_CD\"].map(MAP_SI).fillna(1).astype(int)\n",
    "    df[\"SFDR_PREF_num\"] = df[\"SFDR_PREF\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "    df[\"SFDR_ACTUAL_num\"] = df[\"SFDR_ACTUAL\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "    df[\"TAXONOMYPREF_num\"] = df[\"TAXONOMYPREF\"].map(MAP_TAX).fillna(1).astype(int)\n",
    "\n",
    "    # SFDR engineered\n",
    "    df[\"sfdr_gap\"] = np.clip(df[\"SFDR_PREF_num\"] - df[\"SFDR_ACTUAL_num\"], -2, 2)\n",
    "    df[\"sfdr_opp\"] = np.maximum(df[\"sfdr_gap\"], 0)  # 0..2\n",
    "\n",
    "    # Topics aggregate\n",
    "    topic_cols = [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]\n",
    "    df[\"esg_topics_yes_cnt\"] = df[topic_cols].sum(axis=1)\n",
    "    df[\"topics_norm\"] = df[\"esg_topics_yes_cnt\"] / len(topic_cols)\n",
    "\n",
    "    # Normalized signals (0..1)\n",
    "    df[\"si_norm\"] = np.clip((df[\"SI_CONSIDERATION_num\"] - 1)/2, 0, 1)\n",
    "    df[\"sfdr_norm\"] = np.clip(df[\"sfdr_opp\"]/2, 0, 1)\n",
    "    df[\"tax_norm\"] = np.clip((df[\"TAXONOMYPREF_num\"] - 1)/2, 0, 1)\n",
    "\n",
    "    # PAI block (0..1)\n",
    "    df[\"pai_block\"] = np.where(df[\"PAI_PREF\"]==1, 0.5 + 0.5*df[\"topics_norm\"], 0.0)\n",
    "\n",
    "    # Business logic helper\n",
    "    df[\"si_is_s3\"] = (df[\"SI_CONSIDERATION_num\"] == 3).astype(int)\n",
    "\n",
    "    df[\"si_offering\"] = df[\"si_offering\"].astype(int)\n",
    "    return df\n",
    "\n",
    "df_feat = encode_features(df_id)\n",
    "\n",
    "# Sanity-check distributions\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"sfdr_gap\"], bins=5)\n",
    "plt.title(\"Distribution: sfdr_gap (clipped)\")\n",
    "plt.xlabel(\"sfdr_gap\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"TAXONOMYPREF_num\"], bins=3)\n",
    "plt.title(\"Distribution: TAXONOMYPREF_num (A1/A2/A3 -> 1/2/3)\")\n",
    "plt.xlabel(\"TAXONOMYPREF_num\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_feat[[\"ID\",\"si_offering\",\"MIFID\",\"SI_CONSIDERATION_num\",\"si_is_s3\",\"sfdr_gap\",\"PAI_PREF\",\"TAXONOMYPREF_num\",\"esg_topics_yes_cnt\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b00f99",
   "metadata": {},
   "source": [
    "## 5) Train/Validation split (ID-level)\n",
    "\n",
    "### Question\n",
    "How do we evaluate whether a method will generalize to new clients?\n",
    "\n",
    "### Approach\n",
    "We create a held-out **validation** split (stratified by `si_offering`) so each method is judged fairly:\n",
    "- Train: used to learn data-driven weights and fit ML\n",
    "- Validation: used only for evaluation\n",
    "\n",
    "### Evidence\n",
    "We print:\n",
    "- split sizes\n",
    "- proxy label rates in train vs validation (should be similar)\n",
    "\n",
    "### Decision checkpoint\n",
    "If label rates differ strongly, the split may not be stratified correctly or the dataset is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58719d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_BASE = [\"MIFID\",\"SI_CONSIDERATION_num\",\"si_is_s3\",\"sfdr_norm\",\"pai_block\",\"tax_norm\"]\n",
    "\n",
    "X = df_feat[FEATURES_BASE].copy()\n",
    "y = df_feat[\"si_offering\"].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "idx_train, idx_val = X_train.index, X_val.index\n",
    "\n",
    "print(\"Train:\", len(idx_train), \"Val:\", len(idx_val))\n",
    "print(\"Train si_offering rate:\", y_train.mean().round(4))\n",
    "print(\"Val   si_offering rate:\", y_val.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009ad3f",
   "metadata": {},
   "source": [
    "## 6) Evaluation helpers (metrics + lift + calibration)\n",
    "\n",
    "### What we report (stakeholder meaning)\n",
    "- **AUC:** ranking quality overall (higher is better)\n",
    "- **Average Precision (AP):** ranking quality when positives are rare (often more relevant than AUC)\n",
    "- **Brier score:** probability quality (lower is better)\n",
    "\n",
    "### What we visualize\n",
    "- **Lift by decile:** “Does the top decile have a meaningfully higher SI rate than the bottom?”\n",
    "- **Calibration curve:** “If we treat a score as a probability, is it reliable?”\n",
    "\n",
    "These artifacts support a key stakeholder question:\n",
    "> “If we work the top bucket, do we see a higher hit rate than baseline?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scores(y_true, p, label):\n",
    "    return {\n",
    "        \"model\": label,\n",
    "        \"auc\": roc_auc_score(y_true, p),\n",
    "        \"avg_precision\": average_precision_score(y_true, p),\n",
    "        \"brier\": brier_score_loss(y_true, p)\n",
    "    }\n",
    "\n",
    "def lift_table(y_true, p, n_bins=10):\n",
    "    tmp = pd.DataFrame({\"y\": y_true, \"p\": p})\n",
    "    tmp[\"bin\"] = pd.qcut(tmp[\"p\"], n_bins, labels=False, duplicates=\"drop\") + 1\n",
    "    return tmp.groupby(\"bin\")[\"y\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"si_rate\"})\n",
    "\n",
    "def plot_lift(tab, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(tab.index, tab[\"si_rate\"].values, marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Decile (1=lowest score, 10=highest)\")\n",
    "    plt.ylabel(\"si_offering rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration(y_true, p, title):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed si_offering rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ccbd6",
   "metadata": {},
   "source": [
    "## 7) Method 1 — Fixed rule (stakeholder weights)\n",
    "\n",
    "### Question\n",
    "Can a transparent rule-based score create meaningful lift without any learning?\n",
    "\n",
    "### Approach (your requested logic)\n",
    "- If `MIFID = 0`: score depends **only** on `SI_CONSIDERATION`\n",
    "- If `MIFID = 1`: start from an SI baseline, and **only when `SI = S3`** add confirmation from:\n",
    "  - SFDR opportunity (`sfdr_norm`)\n",
    "  - PAI confirmation (`pai_block`)\n",
    "  - Taxonomy preference (`tax_norm`)\n",
    "\n",
    "We also create **“why columns”** showing component contributions (base, SFDR, PAI, tax).\n",
    "\n",
    "### Evidence\n",
    "On validation we show:\n",
    "- AUC/AP/Brier\n",
    "- Lift by decile\n",
    "- Calibration curve\n",
    "\n",
    "### Decision checkpoint\n",
    "If lift is weak, we do not jump to ML immediately.\n",
    "Next we try a **data-driven weighted rule** that keeps the same structure but learns weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bcf31",
   "metadata": {},
   "source": [
    "**What to look for in the evaluation**\n",
    "- **Lift:** the top decile should have a meaningfully higher SI rate than baseline.\n",
    "- **Calibration:** points near the diagonal indicate “probability-like” behavior.\n",
    "- If lift is weak: we improve the same logic by learning weights (Method 2) before considering ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FixedRuleConfig:\n",
    "    # MIFID=0 branch: only SI\n",
    "    score_A_S1: float = 0\n",
    "    score_A_S2: float = 45\n",
    "    score_A_S3: float = 85\n",
    "\n",
    "    # MIFID=1 branch:\n",
    "    baseline_B_S1: float = 0\n",
    "    baseline_B_S2: float = 30\n",
    "    baseline_B_S3: float = 50\n",
    "\n",
    "    confirm_max: float = 50  # max added if SI is high (S3)\n",
    "    w_sfdr: float = 0.50\n",
    "    w_pai: float = 0.30\n",
    "    w_tax: float = 0.20\n",
    "\n",
    "cfg_fixed = FixedRuleConfig()\n",
    "\n",
    "def score_fixed_rule(df: pd.DataFrame, cfg: FixedRuleConfig) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Baselines\n",
    "    base_A = df[\"SI_CONSIDERATION_num\"].map({1: cfg.score_A_S1, 2: cfg.score_A_S2, 3: cfg.score_A_S3}).astype(float)\n",
    "    base_B = df[\"SI_CONSIDERATION_num\"].map({1: cfg.baseline_B_S1, 2: cfg.baseline_B_S2, 3: cfg.baseline_B_S3}).astype(float)\n",
    "\n",
    "    confirm = cfg.confirm_max * (\n",
    "        cfg.w_sfdr * df[\"sfdr_norm\"] +\n",
    "        cfg.w_pai * df[\"pai_block\"] +\n",
    "        cfg.w_tax * df[\"tax_norm\"]\n",
    "    )\n",
    "\n",
    "    # Apply requested gate: only add confirm when MIFID=1 and SI=S3\n",
    "    score = np.where(df[\"MIFID\"]==0, base_A,\n",
    "                     np.where(df[\"si_is_s3\"]==1, base_B + confirm, base_B))\n",
    "\n",
    "    df[\"score_fixed\"] = np.clip(score, 0, 100)\n",
    "\n",
    "    # Why columns\n",
    "    df[\"why_base\"] = np.where(df[\"MIFID\"]==0, base_A, base_B)\n",
    "    df[\"why_sfdr\"] = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*cfg.w_sfdr*df[\"sfdr_norm\"], 0.0)\n",
    "    df[\"why_pai\"]  = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*cfg.w_pai*df[\"pai_block\"], 0.0)\n",
    "    df[\"why_tax\"]  = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*cfg.w_tax*df[\"tax_norm\"], 0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_scored = score_fixed_rule(df_feat, cfg_fixed)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_scored[\"score_fixed\"], bins=30)\n",
    "plt.title(\"Score distribution: Fixed rule\")\n",
    "plt.xlabel(\"score_fixed\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_scored[[\"score_fixed\",\"why_base\",\"why_sfdr\",\"why_pai\",\"why_tax\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b87d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation performance (Fixed rule)\n",
    "p_fixed = df_scored.loc[idx_val, \"score_fixed\"].values / 100.0\n",
    "fixed_metrics = eval_scores(y_val.values, p_fixed, \"Fixed rule\")\n",
    "display(fixed_metrics)\n",
    "\n",
    "lift_fixed = lift_table(y_val.values, p_fixed)\n",
    "plot_lift(lift_fixed, \"Lift (proxy): Fixed rule\")\n",
    "display(lift_fixed)\n",
    "\n",
    "plot_calibration(y_val.values, p_fixed, \"Calibration (proxy): Fixed rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310e765",
   "metadata": {},
   "source": [
    "## 8) Method 2 — Weighted rule (data-driven, still rule-like)\n",
    "\n",
    "### Question\n",
    "Can we improve the rule by learning weights from data, while keeping the same structure?\n",
    "\n",
    "### Approach\n",
    "We keep the **same gate** and baselines, but learn the confirmation weights:\n",
    "- Train on the subset where confirmation is allowed by design: `MIFID=1` and `SI=S3`\n",
    "- Fit logistic regression using only:\n",
    "  - `sfdr_norm`, `pai_block`, `tax_norm`\n",
    "- Convert positive coefficients into weights that sum to 1 (easy to explain and audit)\n",
    "\n",
    "### Evidence\n",
    "- Bar chart of learned weights\n",
    "- Validation metrics + lift + calibration\n",
    "\n",
    "### Decision checkpoint\n",
    "If weighted rule improves lift and stays stable, it is often the best operational default:\n",
    "- explainable\n",
    "- easy governance\n",
    "- less drift-sensitive than complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede0f93",
   "metadata": {},
   "source": [
    "**Why this step is persuasive**\n",
    "This step preserves the stakeholder rule structure but uses data to answer:\n",
    "> “Which confirmation signal (SFDR vs PAI vs Taxonomy) actually correlates more with SI membership in our data?”\n",
    "\n",
    "If the learned weights are stable over time, this becomes a strong operational default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_scored.loc[idx_train].copy()\n",
    "train_gate = train_df[(train_df[\"MIFID\"]==1) & (train_df[\"si_is_s3\"]==1)].copy()\n",
    "\n",
    "WEIGHT_FEATURES = [\"sfdr_norm\",\"pai_block\",\"tax_norm\"]\n",
    "if train_gate[\"si_offering\"].nunique() < 2:\n",
    "    print(\"Warning: gated training subset has only one class; falling back to fixed weights.\")\n",
    "    w_learned = pd.Series([cfg_fixed.w_sfdr, cfg_fixed.w_pai, cfg_fixed.w_tax], index=WEIGHT_FEATURES)\n",
    "else:\n",
    "    lr_w = LogisticRegression(max_iter=6000, class_weight=\"balanced\")\n",
    "    lr_w.fit(train_gate[WEIGHT_FEATURES], train_gate[\"si_offering\"])\n",
    "    coef = pd.Series(lr_w.coef_[0], index=WEIGHT_FEATURES)\n",
    "\n",
    "    pos = np.maximum(coef.values, 0)  # stakeholder-friendly monotonic assumption\n",
    "    if pos.sum() == 0:\n",
    "        pos = np.ones_like(pos)\n",
    "    w_learned = pd.Series(pos / pos.sum(), index=WEIGHT_FEATURES)\n",
    "\n",
    "display(w_learned.to_frame(\"learned_weight\"))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(w_learned.index, w_learned.values)\n",
    "plt.title(\"Weighted rule: learned confirmation weights (sum=1)\")\n",
    "plt.ylabel(\"weight\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d29102",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WeightedRuleConfig:\n",
    "    score_A_S1: float = cfg_fixed.score_A_S1\n",
    "    score_A_S2: float = cfg_fixed.score_A_S2\n",
    "    score_A_S3: float = cfg_fixed.score_A_S3\n",
    "\n",
    "    baseline_B_S1: float = cfg_fixed.baseline_B_S1\n",
    "    baseline_B_S2: float = cfg_fixed.baseline_B_S2\n",
    "    baseline_B_S3: float = cfg_fixed.baseline_B_S3\n",
    "\n",
    "    confirm_max: float = cfg_fixed.confirm_max\n",
    "\n",
    "cfg_weighted = WeightedRuleConfig()\n",
    "\n",
    "def score_weighted_rule(df: pd.DataFrame, cfg: WeightedRuleConfig, w: pd.Series) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    base_A = df[\"SI_CONSIDERATION_num\"].map({1: cfg.score_A_S1, 2: cfg.score_A_S2, 3: cfg.score_A_S3}).astype(float)\n",
    "    base_B = df[\"SI_CONSIDERATION_num\"].map({1: cfg.baseline_B_S1, 2: cfg.baseline_B_S2, 3: cfg.baseline_B_S3}).astype(float)\n",
    "\n",
    "    confirm = cfg.confirm_max * (\n",
    "        w[\"sfdr_norm\"] * df[\"sfdr_norm\"] +\n",
    "        w[\"pai_block\"] * df[\"pai_block\"] +\n",
    "        w[\"tax_norm\"] * df[\"tax_norm\"]\n",
    "    )\n",
    "\n",
    "    score = np.where(df[\"MIFID\"]==0, base_A,\n",
    "                     np.where(df[\"si_is_s3\"]==1, base_B + confirm, base_B))\n",
    "\n",
    "    df[\"score_weighted\"] = np.clip(score, 0, 100)\n",
    "\n",
    "    df[\"whyW_base\"] = np.where(df[\"MIFID\"]==0, base_A, base_B)\n",
    "    df[\"whyW_sfdr\"] = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*w[\"sfdr_norm\"]*df[\"sfdr_norm\"], 0.0)\n",
    "    df[\"whyW_pai\"]  = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*w[\"pai_block\"]*df[\"pai_block\"], 0.0)\n",
    "    df[\"whyW_tax\"]  = np.where((df[\"MIFID\"]==1) & (df[\"si_is_s3\"]==1), cfg.confirm_max*w[\"tax_norm\"]*df[\"tax_norm\"], 0.0)\n",
    "    return df\n",
    "\n",
    "df_scored = score_weighted_rule(df_scored, cfg_weighted, w_learned)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_scored[\"score_weighted\"], bins=30)\n",
    "plt.title(\"Score distribution: Weighted rule (data-driven)\")\n",
    "plt.xlabel(\"score_weighted\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_scored[[\"score_weighted\",\"whyW_base\",\"whyW_sfdr\",\"whyW_pai\",\"whyW_tax\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation performance (Weighted rule)\n",
    "p_weighted = df_scored.loc[idx_val, \"score_weighted\"].values / 100.0\n",
    "weighted_metrics = eval_scores(y_val.values, p_weighted, \"Weighted rule (data-driven)\")\n",
    "display(weighted_metrics)\n",
    "\n",
    "lift_weighted = lift_table(y_val.values, p_weighted)\n",
    "plot_lift(lift_weighted, \"Lift (proxy): Weighted rule\")\n",
    "display(lift_weighted)\n",
    "\n",
    "plot_calibration(y_val.values, p_weighted, \"Calibration (proxy): Weighted rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c7a5b",
   "metadata": {},
   "source": [
    "## 9) Method 3 — ML: Calibrated Logistic Regression (controlled enhancement)\n",
    "\n",
    "### Question\n",
    "Can ML outperform the rule approaches while remaining explainable and calibrated?\n",
    "\n",
    "### Approach\n",
    "We use **Calibrated Logistic Regression** because it is:\n",
    "- strong for tabular data\n",
    "- interpretable (coefficients)\n",
    "- produces calibrated probabilities for bucket/threshold decisions\n",
    "\n",
    "**Respecting the business structure**\n",
    "We add interaction (“gate”) features:\n",
    "- `gate = MIFID * si_is_s3`\n",
    "- `gate_sfdr = gate * sfdr_norm`\n",
    "- `gate_pai  = gate * pai_block`\n",
    "- `gate_tax  = gate * tax_norm`\n",
    "\n",
    "This allows ML to learn signal strength where it is logically relevant, aligned with the rule design.\n",
    "\n",
    "### Evidence\n",
    "- Validation metrics + lift + calibration\n",
    "- Coefficients from an uncalibrated LR for interpretability\n",
    "\n",
    "### Decision checkpoint\n",
    "Use ML only if it improves lift materially **and** stays stable/calibrated. Otherwise prefer Weighted rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d583216",
   "metadata": {},
   "source": [
    "**How to interpret ML outputs**\n",
    "- If ML improves AUC/AP but has worse calibration, it can still be useful for ranking—but bucket thresholds should be conservative.\n",
    "- Coefficients are a sanity check: are directions plausible (e.g., higher SFDR opportunity increases score)?\n",
    "- Adopt ML only if it materially improves lift in the highest bucket where ROI concentrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ml_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df[FEATURES_BASE].copy()\n",
    "    X[\"gate\"] = X[\"MIFID\"] * X[\"si_is_s3\"]\n",
    "    X[\"gate_sfdr\"] = X[\"gate\"] * X[\"sfdr_norm\"]\n",
    "    X[\"gate_pai\"]  = X[\"gate\"] * X[\"pai_block\"]\n",
    "    X[\"gate_tax\"]  = X[\"gate\"] * X[\"tax_norm\"]\n",
    "    return X\n",
    "\n",
    "Xtr = make_ml_matrix(df_scored.loc[idx_train])\n",
    "Xva = make_ml_matrix(df_scored.loc[idx_val])\n",
    "\n",
    "lr = LogisticRegression(max_iter=8000, class_weight=\"balanced\")\n",
    "cal_lr = CalibratedClassifierCV(lr, method=\"isotonic\", cv=5)\n",
    "cal_lr.fit(Xtr, y_train)\n",
    "\n",
    "p_ml = cal_lr.predict_proba(Xva)[:,1]\n",
    "ml_metrics = eval_scores(y_val.values, p_ml, \"ML: Calibrated Logistic Regression\")\n",
    "display(ml_metrics)\n",
    "\n",
    "lift_ml = lift_table(y_val.values, p_ml)\n",
    "plot_lift(lift_ml, \"Lift (proxy): ML Calibrated Logistic Regression\")\n",
    "display(lift_ml)\n",
    "\n",
    "plot_calibration(y_val.values, p_ml, \"Calibration (proxy): ML Calibrated Logistic Regression\")\n",
    "\n",
    "# Interpretability: coefficients from uncalibrated LR on same features\n",
    "lr_plain = LogisticRegression(max_iter=8000, class_weight=\"balanced\")\n",
    "lr_plain.fit(Xtr, y_train)\n",
    "coef = pd.Series(lr_plain.coef_[0], index=Xtr.columns).sort_values(key=np.abs, ascending=False)\n",
    "display(coef.to_frame(\"coef\"))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(coef.index[:12], coef.values[:12])\n",
    "plt.title(\"Top coefficients (uncalibrated LR; sign indicates direction)\")\n",
    "plt.ylabel(\"coef\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f8fa5",
   "metadata": {},
   "source": [
    "## 10) Validation comparison (Fixed vs Weighted vs ML)\n",
    "\n",
    "### Question\n",
    "Which method is best for prioritization on held-out data?\n",
    "\n",
    "### Approach\n",
    "We compare all three methods on the **same validation set**:\n",
    "- Fixed rule\n",
    "- Weighted rule\n",
    "- Calibrated LR (ML)\n",
    "\n",
    "### Evidence\n",
    "- Table: AUC / AP / Brier for all methods\n",
    "- Bar charts for the three metrics\n",
    "\n",
    "### Decision\n",
    "Choose the method that offers the best trade-off between:\n",
    "- uplift (business value)\n",
    "- explainability (governance)\n",
    "- stability (risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([fixed_metrics, weighted_metrics, ml_metrics]).round(4)\n",
    "display(comparison.sort_values(\"auc\", ascending=False))\n",
    "\n",
    "for metric in [\"auc\",\"avg_precision\",\"brier\"]:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(comparison[\"model\"], comparison[metric])\n",
    "    plt.title(f\"Validation comparison: {metric}\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b2a94",
   "metadata": {},
   "source": [
    "## 11) Operational output: Top 20 IDs with `si_offering=0` + “why” columns\n",
    "\n",
    "### Question\n",
    "How do we turn scores into an actionable list?\n",
    "\n",
    "### Approach\n",
    "- Filter to `si_offering = 0`\n",
    "- Rank by selected method (Fixed or Weighted are available for all IDs)\n",
    "- Add percentiles and 3 buckets (Low / Average / High)\n",
    "- Include “why columns” so the front office understands *why* a client is prioritized\n",
    "\n",
    "### Evidence\n",
    "A top-20 table that can be exported to CRM or used directly for outreach planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK_METHOD = \"score_weighted\"  # \"score_fixed\" or \"score_weighted\"\n",
    "\n",
    "df_out = df_scored.copy()\n",
    "df_out[\"rank_score\"] = df_out[RANK_METHOD]\n",
    "df_out[\"score_percentile\"] = (df_out[\"rank_score\"].rank(pct=True) * 100).round(2)\n",
    "df_out[\"bucket_3\"] = pd.cut(df_out[\"score_percentile\"], bins=[-0.01, 50, 80, 100], labels=[\"Low\",\"Average\",\"High\"])\n",
    "\n",
    "targets = df_out[df_out[\"si_offering\"]==0].sort_values(\"rank_score\", ascending=False)\n",
    "\n",
    "if RANK_METHOD == \"score_fixed\":\n",
    "    why_cols = [\"why_base\",\"why_sfdr\",\"why_pai\",\"why_tax\"]\n",
    "else:\n",
    "    why_cols = [\"whyW_base\",\"whyW_sfdr\",\"whyW_pai\",\"whyW_tax\"]\n",
    "\n",
    "cols = [\"ID\",\"rank_score\",\"score_percentile\",\"bucket_3\",\n",
    "        \"MIFID\",\"SI_CONSIDERATION_num\",\"sfdr_gap\",\"PAI_PREF\",\"TAXONOMYPREF_num\",\"esg_topics_yes_cnt\"] + why_cols\n",
    "\n",
    "targets[cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ce856",
   "metadata": {},
   "source": [
    "## 12) Risks & governance\n",
    "\n",
    "### Core risks\n",
    "- **Proxy label bias:** `si_offering` is membership, not true interest.\n",
    "- **Hard-gate coverage risk:** SFDR/PAI/Tax only influence the score when `SI=S3`. This can miss “emerging” interest.\n",
    "- **Missing data:** defaulting to lowest tier can under-score clients with incomplete questionnaires.\n",
    "\n",
    "### Governance controls\n",
    "- Track missingness and score distributions monthly\n",
    "- Monitor lift-by-decile on recent data\n",
    "- Refresh learned weights on a fixed cadence (e.g., quarterly) or after major process changes\n",
    "- Keep a human-review step for the highest-risk edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085910f",
   "metadata": {},
   "source": [
    "## 13) Pilot plan (to create true labels)\n",
    "\n",
    "### Why we need a pilot\n",
    "The proxy label is useful for benchmarking methods, but stakeholders ultimately want:\n",
    "> “Does targeting the high bucket produce more SI adoption or pipeline?”\n",
    "\n",
    "### Recommended pilot design\n",
    "- Population: `si_offering = 0`\n",
    "- Treatment: top bucket (e.g., top 20% by Weighted rule)\n",
    "- Control: randomized sample from the remaining eligible population (or next bucket)\n",
    "- Outcomes to capture:\n",
    "  - outreach response\n",
    "  - meeting booked\n",
    "  - SI adoption / mandate change\n",
    "  - pipeline created\n",
    "\n",
    "### Decision\n",
    "After the pilot produces true outcomes, retrain and re-compare:\n",
    "Fixed vs Weighted vs ML on **true labels**, not proxy membership."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
