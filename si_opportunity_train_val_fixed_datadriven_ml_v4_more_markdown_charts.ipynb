{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf85a949",
   "metadata": {},
   "source": [
    "# SI Opportunity Scoring — Train/Validation Comparison (v3: +Taxonomy)\n",
    "**Fixed rule → Data-driven rule → ML (Calibrated Logistic Regression)**  \n",
    "**Last updated:** 2026-02-24\n",
    "\n",
    "## Target (proxy) definition\n",
    "We derive **`si_offering`** from `OFFERING_NAME`:\n",
    "- `si_offering_row = 1` if OFFERING_NAME contains token **SI** (case-insensitive)\n",
    "- `si_offering (per ID) = max(si_offering_row)` across rows for that client\n",
    "\n",
    "## Objective\n",
    "Rank **IDs with `si_offering = 0`** (not currently in an SI offering) by predicted SI alignment from preference fields.\n",
    "\n",
    "## New requirement\n",
    "For the branch with SFDR (MiFID=1), we also include **Taxonomy preference**:\n",
    "- `TAXONOMYPREF`: A1, A2, A3 (mapped to 1,2,3)\n",
    "\n",
    "## Model comparison on the same validation set\n",
    "We compare three approaches:\n",
    "1) **Fixed-weight rule** (transparent business logic)  \n",
    "2) **Data-driven rule** (learn branch-B weights statistically on train, apply to validation)  \n",
    "3) **ML**: **Calibrated Logistic Regression** (train → validate)\n",
    "\n",
    "> Note: `si_offering` is a proxy label (membership ≠ true interest). Use the pilot plan to create true outcome labels.\n",
    "\n",
    "**v4 update:** Adds stakeholder narrative + additional charts (distributions, bucket rates, calibration, coefficients, metric comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc68815",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b5682",
   "metadata": {},
   "source": [
    "### How to read this notebook (stakeholder-friendly)\n",
    "Each section follows the same pattern:\n",
    "\n",
    "1) **Question** we need to answer  \n",
    "2) **Logic** (short, business terms)  \n",
    "3) **Evidence** (a chart/table)  \n",
    "4) **Decision / next step**\n",
    "\n",
    "Code exists only to generate the artifact that supports the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121c912",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Load data (fallback demo) + shuffle raw rows\n",
    "\n",
    "**Expected columns**:\n",
    "- `ID`, `IO_TYPE`, `LIFE_CYCLE`, `OFFERING_NAME`\n",
    "- `SI_CONSIDERATION_CD`, `SFDR_PREF`, `SFDR_ACTUAL`, `PAI_PREF`, `MIFID`, `TAXONOMYPREF`\n",
    "- ESG topics: `GHG`, `Biodiversity`, `Water`, `Waste`, `Social`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582cf41",
   "metadata": {},
   "source": [
    "### Why we shuffle raw rows\n",
    "Many operational datasets arrive **sorted** (by ID, by date, by system order).  \n",
    "Shuffling avoids accidental ordering artifacts and makes the pipeline more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data.csv\")  # <-- change to your real file path\n",
    "\n",
    "def make_synthetic_data(n=9000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": rng.integers(1, n//2 + 1, size=n),  # multiple rows per ID\n",
    "        \"IO_TYPE\": rng.choice([\"normal\", \"zombie\"], size=n, p=[0.97, 0.03]),\n",
    "        \"LIFE_CYCLE\": rng.choice([\"open\", \"closed\"], size=n, p=[0.9, 0.1]),\n",
    "\n",
    "        \"SI_CONSIDERATION_CD\": rng.choice([\"S1\",\"S2\",\"S3\", None], size=n, p=[0.35,0.35,0.2,0.1]),\n",
    "        \"SFDR_PREF\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.4,0.35,0.2,0.05]),\n",
    "        \"SFDR_ACTUAL\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.45,0.35,0.15,0.05]),\n",
    "        \"PAI_PREF\": rng.choice([\"PAI Selected\", None], size=n, p=[0.3,0.7]),\n",
    "        \"MIFID\": rng.choice([\"Yes\",\"No\", None], size=n, p=[0.55,0.4,0.05]),\n",
    "        \"TAXONOMYPREF\": rng.choice([\"A1\",\"A2\",\"A3\", None], size=n, p=[0.5,0.35,0.1,0.05]),\n",
    "\n",
    "        \"GHG\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.25,0.65,0.05,0.05]),\n",
    "        \"Biodiversity\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.2,0.7,0.05,0.05]),\n",
    "        \"Water\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.22,0.68,0.05,0.05]),\n",
    "        \"Waste\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.18,0.72,0.05,0.05]),\n",
    "        \"Social\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.28,0.62,0.05,0.05]),\n",
    "    })\n",
    "\n",
    "    df[\"OFFERING_NAME\"] = rng.choice(\n",
    "        [\"Core\", \"Standard\", \"ESG Plus\", \"SI Focus\", \"Core SI\", \"Income\", \"SI Sustainable\"],\n",
    "        size=n, p=[0.25,0.25,0.15,0.15,0.08,0.07,0.05]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded: {DATA_PATH}  shape={df_raw.shape}\")\n",
    "else:\n",
    "    df_raw = make_synthetic_data()\n",
    "    print(\"DATA_PATH not found; using synthetic demo dataset.\")\n",
    "    print(f\"shape={df_raw.shape}\")\n",
    "\n",
    "# Reproducible shuffle of raw rows (avoids ordering artifacts)\n",
    "df_raw = df_raw.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f329e45",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Derive `si_offering` from name and aggregate to ID-level\n",
    "\n",
    "We operate at **ID-level** to avoid recommending the same client multiple times.\n",
    "\n",
    "**Aggregation rule (transparent defaults):**\n",
    "- `si_offering`: max across rows\n",
    "- preference fields: mode (or first if tie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda42b2",
   "metadata": {},
   "source": [
    "### Why aggregate to ID-level\n",
    "We recommend **clients/IDs**, not rows. If an ID appears multiple times (multiple records),\n",
    "row-level scoring can inflate evidence and produce duplicates.\n",
    "\n",
    "So we aggregate to a **single record per ID** before scoring and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd27978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart: row-level vs ID-level size + proxy label prevalence (before cleaning)\n",
    "sizes = pd.DataFrame({\n",
    "    \"level\": [\"row-level\", \"ID-level\"],\n",
    "    \"rows\": [len(df), len(df_id)],\n",
    "    \"si_offering_rate\": [df[\"si_offering_row\"].mean(), df_id[\"si_offering\"].mean()]\n",
    "})\n",
    "display(sizes)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(sizes[\"level\"], sizes[\"rows\"])\n",
    "plt.title(\"Dataset size: row-level vs ID-level\")\n",
    "plt.ylabel(\"Number of records\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(sizes[\"level\"], sizes[\"si_offering_rate\"])\n",
    "plt.title(\"Proxy label prevalence: si_offering rate\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df[\"si_offering_row\"] = df[\"OFFERING_NAME\"].astype(str).str.contains(r\"\\bSI\\b\", case=False, na=False).astype(int)\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    if len(s2) == 0:\n",
    "        return np.nan\n",
    "    m = s2.mode()\n",
    "    if len(m) > 0:\n",
    "        return m.iloc[0]\n",
    "    return s2.iloc[0]\n",
    "\n",
    "agg_dict = {\n",
    "    \"IO_TYPE\": mode_or_first,\n",
    "    \"LIFE_CYCLE\": mode_or_first,\n",
    "    \"SI_CONSIDERATION_CD\": mode_or_first,\n",
    "    \"SFDR_PREF\": mode_or_first,\n",
    "    \"SFDR_ACTUAL\": mode_or_first,\n",
    "    \"PAI_PREF\": mode_or_first,\n",
    "    \"MIFID\": mode_or_first,\n",
    "    \"TAXONOMYPREF\": mode_or_first,\n",
    "    \"GHG\": mode_or_first,\n",
    "    \"Biodiversity\": mode_or_first,\n",
    "    \"Water\": mode_or_first,\n",
    "    \"Waste\": mode_or_first,\n",
    "    \"Social\": mode_or_first,\n",
    "    \"si_offering_row\": \"max\",\n",
    "}\n",
    "\n",
    "df_id = df.groupby(\"ID\", as_index=False).agg(agg_dict).rename(columns={\"si_offering_row\":\"si_offering\"})\n",
    "\n",
    "print(\"Row-level rows:\", len(df))\n",
    "print(\"ID-level rows :\", len(df_id))\n",
    "print(\"si_offering rate (ID-level):\", df_id[\"si_offering\"].mean().round(4))\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa087fef",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Cleaning rules\n",
    "\n",
    "- Remove `IO_TYPE='zombie'`\n",
    "- Keep `LIFE_CYCLE='open'`\n",
    "- Convert topics/MiFID/PAI to binary\n",
    "- Keep missing for ordinal/categorical as 'nan' then default conservatively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf415768",
   "metadata": {},
   "source": [
    "### Why cleaning is critical\n",
    "- Removes **non-actionable** IDs (zombie / closed lifecycle)\n",
    "- Standardizes messy values (`--`, nulls)\n",
    "- Prevents unstable scoring due to inconsistent encodings\n",
    "\n",
    "We also inspect missingness to make assumptions explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness report (post-cleaning) for key fields\n",
    "key_cols = [\"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\",\"TAXONOMYPREF\",\"MIFID\",\"PAI_PREF\",\n",
    "            \"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]\n",
    "miss = df_clean[key_cols].isna().mean().sort_values(ascending=False)\n",
    "display(miss.to_frame(\"missing_rate\").head(10))\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.bar(miss.index[:10], miss.values[:10])\n",
    "plt.title(\"Top missing rates (post-cleaning) — key fields\")\n",
    "plt.ylabel(\"Missing rate\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_to_1(x):\n",
    "    if pd.isna(x): return 0\n",
    "    x = str(x).strip()\n",
    "    if x == \"--\": return 0\n",
    "    return 1 if x.lower() == \"yes\" else 0\n",
    "\n",
    "def clean_id_level(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df[df[\"IO_TYPE\"].fillna(\"\").str.lower() != \"zombie\"]\n",
    "    df = df[df[\"LIFE_CYCLE\"].fillna(\"\").str.lower() == \"open\"]\n",
    "\n",
    "    for c in [\"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\",\"TAXONOMYPREF\"]:\n",
    "        df[c] = df[c].astype(\"object\").where(df[c].notna(), \"nan\")\n",
    "\n",
    "    for c in [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]:\n",
    "        df[c] = df[c].apply(yes_to_1).astype(int)\n",
    "\n",
    "    df[\"MIFID\"] = df[\"MIFID\"].apply(yes_to_1).astype(int)\n",
    "    df[\"PAI_PREF\"] = (df[\"PAI_PREF\"].astype(str).str.lower() == \"pai selected\").astype(int)\n",
    "    df[\"si_offering\"] = df[\"si_offering\"].astype(int)\n",
    "    return df\n",
    "\n",
    "df_clean = clean_id_level(df_id)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"stage\": [\"before\", \"after\"],\n",
    "    \"rows\": [len(df_id), len(df_clean)],\n",
    "    \"si_offering_rate\": [df_id[\"si_offering\"].mean(), df_clean[\"si_offering\"].mean()]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace8d0d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Feature engineering (including SFDR gap + Taxonomy mapping)\n",
    "\n",
    "We avoid OFFERING_NAME-derived features (to prevent leakage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506ca34",
   "metadata": {},
   "source": [
    "### Sanity-check engineered signals\n",
    "Before we score anything, we check that key engineered signals are populated and reasonable:\n",
    "- SFDR gap distribution\n",
    "- Taxonomy distribution and normalized scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"sfdr_gap\"], bins=5)\n",
    "plt.title(\"Distribution of SFDR gap (clipped)\")\n",
    "plt.xlabel(\"sfdr_gap\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"TAXONOMYPREF_num\"], bins=3)\n",
    "plt.title(\"Distribution of Taxonomy preference (A1/A2/A3 mapped to 1/2/3)\")\n",
    "plt.xlabel(\"TAXONOMYPREF_num\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"tax_norm\"], bins=10)\n",
    "plt.title(\"Distribution of tax_norm (0..1)\")\n",
    "plt.xlabel(\"tax_norm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57187d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SI = {\"S1\":1, \"S2\":2, \"S3\":3, \"nan\": np.nan}\n",
    "MAP_SFDR = {\"F1\":1, \"F2\":2, \"F3\":3, \"nan\": np.nan}\n",
    "MAP_TAX = {\"A1\":1, \"A2\":2, \"A3\":3, \"nan\": np.nan}\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"SI_CONSIDERATION_num\"] = df[\"SI_CONSIDERATION_CD\"].map(MAP_SI).fillna(1).astype(int)\n",
    "    df[\"SFDR_PREF_num\"] = df[\"SFDR_PREF\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "    df[\"SFDR_ACTUAL_num\"] = df[\"SFDR_ACTUAL\"].map(MAP_SFDR).fillna(1).astype(int)\n",
    "    df[\"TAXONOMYPREF_num\"] = df[\"TAXONOMYPREF\"].map(MAP_TAX).fillna(1).astype(int)\n",
    "\n",
    "    df[\"sfdr_gap\"] = np.clip(df[\"SFDR_PREF_num\"] - df[\"SFDR_ACTUAL_num\"], -2, 2)\n",
    "    df[\"sfdr_opp\"] = np.maximum(df[\"sfdr_gap\"], 0)  # 0..2\n",
    "\n",
    "    topic_cols = [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]\n",
    "    df[\"esg_topics_yes_cnt\"] = df[topic_cols].sum(axis=1)\n",
    "    df[\"esg_topics_yes_share\"] = df[\"esg_topics_yes_cnt\"] / len(topic_cols)\n",
    "\n",
    "    df[\"si_norm\"] = np.clip((df[\"SI_CONSIDERATION_num\"] - 1)/2, 0, 1)\n",
    "    df[\"sfdr_norm\"] = np.clip(df[\"sfdr_opp\"]/2, 0, 1)\n",
    "    df[\"topics_norm\"] = np.clip(df[\"esg_topics_yes_share\"], 0, 1)\n",
    "    df[\"topics_if_pai\"] = df[\"topics_norm\"] * df[\"PAI_PREF\"]\n",
    "    df[\"tax_norm\"] = np.clip((df[\"TAXONOMYPREF_num\"] - 1)/2, 0, 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = engineer_features(df_clean)\n",
    "df_feat[[\"ID\",\"si_offering\",\"MIFID\",\"sfdr_gap\",\"PAI_PREF\",\"TAXONOMYPREF_num\",\"tax_norm\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2097c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Fixed-weight rule score (branching, includes Taxonomy in SFDR branch)\n",
    "\n",
    "**Logic:**\n",
    "- If `MIFID=0`: SI-only (capped)\n",
    "- If `MIFID=1`: combine **SFDR opportunity + PAI block + Taxonomy**\n",
    "\n",
    "**Default weights (Branch B):** 60% SFDR, 25% PAI block, 15% Taxonomy (sum=100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d88dd1",
   "metadata": {},
   "source": [
    "### Fixed-weight rule: what it represents\n",
    "This is the most transparent method: it encodes stakeholder logic directly.\n",
    "\n",
    "- If **MiFID=0**: SI-only (capped)\n",
    "- If **MiFID=1**: SFDR opportunity + PAI/topics + Taxonomy\n",
    "\n",
    "Next we validate it out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution for fixed rule\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"score_fixed\"], bins=30)\n",
    "plt.title(\"Score distribution: Fixed-weight rule\")\n",
    "plt.xlabel(\"score_fixed\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Proxy success by broad buckets (overall, before split)\n",
    "tmp = df_feat.copy()\n",
    "tmp[\"bucket\"] = pd.qcut(tmp[\"score_fixed\"], 3, labels=[\"Low\",\"Average\",\"High\"], duplicates=\"drop\")\n",
    "bucket_rate = tmp.groupby(\"bucket\")[\"si_offering\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"si_rate\"})\n",
    "display(bucket_rate)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(bucket_rate.index.astype(str), bucket_rate[\"si_rate\"].values)\n",
    "plt.title(\"Proxy evidence: si_offering rate by Fixed-rule bucket\")\n",
    "plt.ylabel(\"si_offering rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17deb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RuleConfig:\n",
    "    si_score_s1: float = 20\n",
    "    si_score_s2: float = 50\n",
    "    si_score_s3: float = 80\n",
    "    w_sfdr: float = 0.60\n",
    "    w_pai_block: float = 0.25\n",
    "    w_tax: float = 0.15\n",
    "\n",
    "cfg = RuleConfig()\n",
    "\n",
    "def score_fixed_rule(df: pd.DataFrame, cfg: RuleConfig) -> pd.Series:\n",
    "    si_score = df[\"SI_CONSIDERATION_num\"].map({1: cfg.si_score_s1, 2: cfg.si_score_s2, 3: cfg.si_score_s3}).astype(float)\n",
    "    pai_block = np.where(df[\"PAI_PREF\"] == 1, 0.5 + 0.5*df[\"topics_norm\"], 0.0)  # 0..1\n",
    "\n",
    "    score_B = 100 * (\n",
    "        cfg.w_sfdr * df[\"sfdr_norm\"] +\n",
    "        cfg.w_pai_block * pai_block +\n",
    "        cfg.w_tax * df[\"tax_norm\"]\n",
    "    )\n",
    "\n",
    "    score = np.where(df[\"MIFID\"]==1, score_B, si_score)\n",
    "    return pd.Series(np.clip(score, 0, 100), index=df.index)\n",
    "\n",
    "df_feat[\"score_fixed\"] = score_fixed_rule(df_feat, cfg)\n",
    "df_feat[\"score_fixed\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9969c",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Train/Validation split (ID-level)\n",
    "\n",
    "We use a stratified split on `si_offering` (proxy label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719c39f",
   "metadata": {},
   "source": [
    "### Why validation matters\n",
    "All three methods are judged on the **same validation set**:\n",
    "\n",
    "- Fixed-weight rule\n",
    "- Data-driven rule (learn on train → apply on validation)\n",
    "- ML (train → validation)\n",
    "\n",
    "We report:\n",
    "- **AUC** (ranking)\n",
    "- **Average Precision** (ranking for imbalanced targets)\n",
    "- **Brier** (probability quality)\n",
    "and show **lift-by-decile** charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_ALL = [\n",
    "    \"si_norm\",\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\",\"tax_norm\",\n",
    "    \"esg_topics_yes_cnt\",\"sfdr_gap\",\"MIFID\",\"SI_CONSIDERATION_num\",\"TAXONOMYPREF_num\"\n",
    "]\n",
    "\n",
    "X = df_feat[FEATURES_ALL].copy()\n",
    "y = df_feat[\"si_offering\"].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "idx_train, idx_val = X_train.index, X_val.index\n",
    "\n",
    "print(\"Train size:\", len(idx_train), \"Val size:\", len(idx_val))\n",
    "print(\"Train si_offering rate:\", y_train.mean().round(4), \"Val si_offering rate:\", y_val.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51f8a0",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scores(y_true, p, label):\n",
    "    return {\n",
    "        \"model\": label,\n",
    "        \"auc\": roc_auc_score(y_true, p),\n",
    "        \"avg_precision\": average_precision_score(y_true, p),\n",
    "        \"brier\": brier_score_loss(y_true, p)\n",
    "    }\n",
    "\n",
    "def lift_table(y_true, p, n_bins=10):\n",
    "    tmp = pd.DataFrame({\"y\": y_true, \"p\": p})\n",
    "    tmp[\"bin\"] = pd.qcut(tmp[\"p\"], n_bins, labels=False, duplicates=\"drop\") + 1\n",
    "    return tmp.groupby(\"bin\")[\"y\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"si_rate\"})\n",
    "\n",
    "def plot_lift(tab, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(tab.index, tab[\"si_rate\"].values, marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Decile (1=lowest, 10=highest)\")\n",
    "    plt.ylabel(\"si_offering rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928abeb",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Fixed-weight rule — validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fixed_val = df_feat.loc[idx_val, \"score_fixed\"].values / 100.0\n",
    "fixed_metrics = eval_scores(y_val.values, p_fixed_val, \"Fixed-weight rule (+Taxonomy)\")\n",
    "fixed_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96e161",
   "metadata": {},
   "source": [
    "### Calibration (proxy): Fixed-weight rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(y_val.values, p_fixed_val, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.title(\"Calibration (proxy): Fixed-weight rule\")\n",
    "plt.xlabel(\"Predicted (score_fixed / 100)\")\n",
    "plt.ylabel(\"Observed si_offering rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce046068",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_fixed = lift_table(y_val.values, p_fixed_val)\n",
    "plot_lift(lift_fixed, \"Lift (proxy): Fixed-weight rule (+Taxonomy)\")\n",
    "lift_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330dfe9",
   "metadata": {},
   "source": [
    "---\n",
    "## 9) Data-driven rule — learn weights on train (branch B) and apply on validation\n",
    "\n",
    "We keep stakeholder interpretability by learning weights for the **SFDR branch only**.\n",
    "\n",
    "- Branch A (MiFID=0): keep the capped SI mapping\n",
    "- Branch B (MiFID=1): fit logistic regression on train subset and convert coefficients to weights summing to 100\n",
    "  using features: `sfdr_norm`, `PAI_PREF`, `topics_if_pai`, `tax_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = df_feat.loc[idx_train]\n",
    "train_B = train_B[train_B[\"MIFID\"] == 1].copy()\n",
    "\n",
    "DD_B_FEATURES = [\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\",\"tax_norm\"]\n",
    "\n",
    "lr_dd_B = LogisticRegression(max_iter=4000, class_weight=\"balanced\")\n",
    "lr_dd_B.fit(train_B[DD_B_FEATURES], train_B[\"si_offering\"])\n",
    "\n",
    "coef_B = pd.Series(lr_dd_B.coef_[0], index=DD_B_FEATURES).sort_values(key=np.abs, ascending=False)\n",
    "coef_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_to_100_weights(coef_series):\n",
    "    pos = np.maximum(coef_series.values, 0)\n",
    "    if pos.sum() == 0:\n",
    "        pos = np.ones_like(pos)\n",
    "    w = 100 * pos / pos.sum()\n",
    "    return pd.Series(w, index=coef_series.index).sort_values(ascending=False)\n",
    "\n",
    "w_dd_B = coef_to_100_weights(coef_B)\n",
    "w_dd_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ec251",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(w_dd_B.index, w_dd_B.values)\n",
    "plt.title(\"Learned Branch-B weights (sum=100)\")\n",
    "plt.ylabel(\"weight\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat[\"score_datadriven\"] = np.nan\n",
    "\n",
    "# Branch A (MIFID=0): fixed SI mapping\n",
    "si_score = df_feat[\"SI_CONSIDERATION_num\"].map({1: cfg.si_score_s1, 2: cfg.si_score_s2, 3: cfg.si_score_s3}).astype(float)\n",
    "df_feat.loc[df_feat[\"MIFID\"]==0, \"score_datadriven\"] = si_score[df_feat[\"MIFID\"]==0]\n",
    "\n",
    "# Branch B (MIFID=1): learned weights\n",
    "scoreB = (\n",
    "    w_dd_B[\"sfdr_norm\"] * df_feat[\"sfdr_norm\"] +\n",
    "    w_dd_B[\"PAI_PREF\"] * df_feat[\"PAI_PREF\"] +\n",
    "    w_dd_B[\"topics_if_pai\"] * df_feat[\"topics_if_pai\"] +\n",
    "    w_dd_B[\"tax_norm\"] * df_feat[\"tax_norm\"]\n",
    ")\n",
    "\n",
    "df_feat.loc[df_feat[\"MIFID\"]==1, \"score_datadriven\"] = scoreB[df_feat[\"MIFID\"]==1]\n",
    "df_feat[\"score_datadriven\"] = df_feat[\"score_datadriven\"].clip(0,100)\n",
    "\n",
    "p_dd_val = df_feat.loc[idx_val, \"score_datadriven\"].values / 100.0\n",
    "dd_metrics = eval_scores(y_val.values, p_dd_val, \"Data-driven rule (learned Branch-B weights)\")\n",
    "dd_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671016f2",
   "metadata": {},
   "source": [
    "### Calibration (proxy): Data-driven rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7392dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true, prob_pred = calibration_curve(y_val.values, p_dd_val, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.title(\"Calibration (proxy): Data-driven rule\")\n",
    "plt.xlabel(\"Predicted (score_datadriven / 100)\")\n",
    "plt.ylabel(\"Observed si_offering rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d382e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_dd = lift_table(y_val.values, p_dd_val)\n",
    "plot_lift(lift_dd, \"Lift (proxy): Data-driven rule (+Taxonomy)\")\n",
    "lift_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa3a38",
   "metadata": {},
   "source": [
    "---\n",
    "## 10) ML — Calibrated Logistic Regression (train → validate)\n",
    "\n",
    "We train a single calibrated logistic regression on the full train split using engineered features (including Taxonomy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a769f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_FEATURES = [\n",
    "    \"si_norm\",\"sfdr_norm\",\"PAI_PREF\",\"topics_if_pai\",\"tax_norm\",\n",
    "    \"esg_topics_yes_cnt\",\"sfdr_gap\",\"MIFID\",\"SI_CONSIDERATION_num\",\"TAXONOMYPREF_num\"\n",
    "]\n",
    "\n",
    "Xtr = X_train[ML_FEATURES].copy()\n",
    "Xva = X_val[ML_FEATURES].copy()\n",
    "\n",
    "lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
    "cal_lr = CalibratedClassifierCV(lr, method=\"isotonic\", cv=5)\n",
    "cal_lr.fit(Xtr, y_train)\n",
    "\n",
    "p_lr_val = cal_lr.predict_proba(Xva)[:,1]\n",
    "ml_metrics = eval_scores(y_val.values, p_lr_val, \"ML: Calibrated Logistic Regression\")\n",
    "ml_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e8960",
   "metadata": {},
   "source": [
    "### ML interpretability: coefficient overview (trained on train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d87dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit uncalibrated LR on train for coefficient inspection (calibration wraps multiple CV estimators)\n",
    "lr_plain = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
    "lr_plain.fit(Xtr, y_train)\n",
    "\n",
    "coef = pd.Series(lr_plain.coef_[0], index=ML_FEATURES).sort_values(key=np.abs, ascending=False)\n",
    "display(coef.to_frame(\"coef\"))\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.bar(coef.index[:10], coef.values[:10])\n",
    "plt.title(\"Top coefficients (uncalibrated LR; sign indicates direction)\")\n",
    "plt.ylabel(\"coefficient\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54fcdd",
   "metadata": {},
   "source": [
    "### Calibration (proxy): ML calibrated logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5807791",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true, prob_pred = calibration_curve(y_val.values, p_lr_val, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.title(\"Calibration (proxy): ML Calibrated Logistic Regression\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed si_offering rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05912ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_lr = lift_table(y_val.values, p_lr_val)\n",
    "plot_lift(lift_lr, \"Lift (proxy): ML Calibrated Logistic Regression (+Taxonomy)\")\n",
    "lift_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e193b7f",
   "metadata": {},
   "source": [
    "---\n",
    "## 11) Validation comparison (Fixed vs Data-driven vs ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([fixed_metrics, dd_metrics, ml_metrics]).sort_values(\"auc\", ascending=False).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce7655",
   "metadata": {},
   "source": [
    "### Visual comparison of validation metrics\n",
    "A quick stakeholder view of which approach wins on:\n",
    "- ranking (AUC / Average Precision)\n",
    "- probability quality (Brier; lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([fixed_metrics, dd_metrics, ml_metrics])\n",
    "for metric in [\"auc\", \"avg_precision\", \"brier\"]:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(comparison[\"model\"], comparison[metric])\n",
    "    plt.title(f\"Validation comparison: {metric}\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a6635",
   "metadata": {},
   "source": [
    "---\n",
    "## 12) Operational output: target IDs (`si_offering=0`) ranked by chosen method\n",
    "\n",
    "Operationally, we recommend ranking by a score available for **all IDs**:\n",
    "- `score_fixed` or `score_datadriven`\n",
    "\n",
    "We also assign percentile buckets (Low/Average/High)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK_METHOD = \"score_datadriven\"  # options: \"score_fixed\" or \"score_datadriven\"\n",
    "\n",
    "df_out = df_feat.copy()\n",
    "df_out[\"rank_score\"] = df_out[RANK_METHOD]\n",
    "df_out[\"score_percentile\"] = (df_out[\"rank_score\"].rank(pct=True) * 100).round(2)\n",
    "df_out[\"bucket_3\"] = pd.cut(df_out[\"score_percentile\"], bins=[-0.01, 50, 80, 100], labels=[\"Low\",\"Average\",\"High\"])\n",
    "\n",
    "targets = df_out[df_out[\"si_offering\"]==0].sort_values(\"rank_score\", ascending=False)\n",
    "cols = [\"ID\",\"rank_score\",\"score_percentile\",\"bucket_3\",\n",
    "        \"MIFID\",\"SI_CONSIDERATION_num\",\"sfdr_gap\",\"PAI_PREF\",\"TAXONOMYPREF_num\",\"esg_topics_yes_cnt\"]\n",
    "targets[cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553eb72",
   "metadata": {},
   "source": [
    "---\n",
    "## 13) Pilot plan (create true labels)\n",
    "\n",
    "Because `si_offering` is a proxy, validate impact with a pilot:\n",
    "- Treatment: High bucket (top 20% of `si_offering=0`)\n",
    "- Control: randomized sample from eligible pool (or next bucket)\n",
    "- Outcomes: response, meeting booked, adoption, pipeline created\n",
    "\n",
    "After pilot: retrain and compare Fixed vs Data-driven vs ML using true outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
