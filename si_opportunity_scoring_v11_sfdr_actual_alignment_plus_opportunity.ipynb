{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43300645",
   "metadata": {},
   "source": [
    "# SI Opportunity Scoring — Improving SFDR & PAI Signals (v11)\n",
    "**Why this version exists:** in earlier versions `sfdr_norm = max(pref-actual, 0)` captures **only upgrade opportunity**.  \n",
    "That can miss important information like **`SFDR_ACTUAL = F3`** (already aligned with sustainable products), which is often a *strong positive signal* for SI interest.\n",
    "\n",
    "This notebook updates the feature engineering so we capture **both**:\n",
    "1) **Alignment (current level):** `sfdr_actual_norm`  \n",
    "2) **Opportunity (preference > actual):** `sfdr_opp_norm`\n",
    "\n",
    "We also make `PAI` parsing more robust and add missingness-safe handling so signals don’t collapse to zero due to encoding mismatches.\n",
    "\n",
    "**Last updated:** 2026-02-27\n",
    "\n",
    "---\n",
    "\n",
    "## Business logic (kept)\n",
    "- If `MIFID = 0` → score uses only `SI_CONSIDERATION`\n",
    "- If `MIFID = 1` → score = **alpha · SI + (1−alpha) · Confirmations**\n",
    "  - start with `alpha = 0.80` for the fixed rule\n",
    "  - learn a better alpha (bounded) in the weighted rule\n",
    "\n",
    "## What changes in v11\n",
    "### SFDR signals (new)\n",
    "- `sfdr_actual_norm` in **0..1** (F1=0, F2=0.5, F3=1)\n",
    "- `sfdr_pref_norm` in **0..1**\n",
    "- `sfdr_gap_signed_norm` in **−1..1** (pref-actual, clipped, scaled)\n",
    "- `sfdr_opp_norm` in **0..1** (only positive gap, like before)\n",
    "\n",
    "**Why this matters:**  \n",
    "If a client is already in **F3**, `sfdr_actual_norm=1` retains that positive signal even when there is **no upgrade gap**.\n",
    "\n",
    "### PAI signals (improved)\n",
    "- `pai_selected` is parsed from a wider set of values (not only exact `\"PAI Selected\"`)\n",
    "- `pai_block` remains 0..1 but becomes more robust to data formatting\n",
    "\n",
    "### Missingness (safer)\n",
    "We add explicit missing flags for SFDR fields so “missing” isn’t silently treated as the lowest tier.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods compared\n",
    "1) Fixed rule (alpha=0.80, fixed confirmation weights)  \n",
    "2) Weighted rule (learn confirmation weights + learn alpha within bounds)  \n",
    "3) ML (Calibrated Logistic Regression with MiFID interactions)\n",
    "\n",
    "Primary operational metrics:\n",
    "- Precision/Lift@Top 10% and 20%\n",
    "- Lift-by-decile curve\n",
    "- Calibration curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a42ff8",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bf372",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Load raw data + shuffle rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data.csv\")  # <-- change to your real file path\n",
    "\n",
    "def make_synthetic_data(n=9000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return pd.DataFrame({\n",
    "        \"ID\": rng.integers(1, n//2 + 1, size=n),\n",
    "        \"IO_TYPE\": rng.choice([\"normal\", \"zombie\"], size=n, p=[0.97, 0.03]),\n",
    "        \"LIFE_CYCLE\": rng.choice([\"open\", \"closed\"], size=n, p=[0.9, 0.1]),\n",
    "        \"OFFERING_NAME\": rng.choice(\n",
    "            [\"Core\", \"Standard\", \"ESG Plus\", \"SI Focus\", \"Core SI\", \"Income\", \"SI Sustainable\", None],\n",
    "            size=n, p=[0.23,0.23,0.14,0.14,0.08,0.07,0.06,0.05]\n",
    "        ),\n",
    "        \"SI_CONSIDERATION_CD\": rng.choice([\"S1\",\"S2\",\"S3\", None], size=n, p=[0.35,0.35,0.2,0.1]),\n",
    "        \"SFDR_PREF\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.4,0.35,0.2,0.05]),\n",
    "        \"SFDR_ACTUAL\": rng.choice([\"F1\",\"F2\",\"F3\", None], size=n, p=[0.45,0.35,0.15,0.05]),\n",
    "        \"PAI_PREF\": rng.choice([\"PAI Selected\", \"Yes\", \"No\", None], size=n, p=[0.25,0.1,0.05,0.6]),\n",
    "        \"MIFID\": rng.choice([\"Yes\",\"No\", None], size=n, p=[0.55,0.4,0.05]),\n",
    "        \"TAXONOMYPREF\": rng.choice([\"A1\",\"A2\",\"A3\", None], size=n, p=[0.5,0.35,0.1,0.05]),\n",
    "        \"GHG\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.25,0.65,0.05,0.05]),\n",
    "        \"Biodiversity\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.2,0.7,0.05,0.05]),\n",
    "        \"Water\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.22,0.68,0.05,0.05]),\n",
    "        \"Waste\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.18,0.72,0.05,0.05]),\n",
    "        \"Social\": rng.choice([\"Yes\",\"No\",\"--\", None], size=n, p=[0.28,0.62,0.05,0.05]),\n",
    "    })\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded: {DATA_PATH}  shape={df_raw.shape}\")\n",
    "else:\n",
    "    df_raw = make_synthetic_data()\n",
    "    print(\"DATA_PATH not found; using synthetic demo dataset.\")\n",
    "    print(f\"shape={df_raw.shape}\")\n",
    "\n",
    "df_raw = df_raw.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfa381",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Cleaning & filtering (ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712aadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_RAW = [\n",
    "    \"ID\",\"IO_TYPE\",\"LIFE_CYCLE\",\"OFFERING_NAME\",\n",
    "    \"SI_CONSIDERATION_CD\",\"SFDR_PREF\",\"SFDR_ACTUAL\",\"PAI_PREF\",\"MIFID\",\"TAXONOMYPREF\",\n",
    "    \"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"\n",
    "]\n",
    "missing_cols = [c for c in REQUIRED_RAW if c not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required raw columns: {missing_cols}\")\n",
    "\n",
    "def clean_filter_only(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df = df.replace({\"--\": np.nan, \"\": np.nan})\n",
    "\n",
    "    before = len(df)\n",
    "    df = df[df[\"IO_TYPE\"].fillna(\"\").str.lower() != \"zombie\"]\n",
    "    after_zombie = len(df)\n",
    "    df = df[df[\"LIFE_CYCLE\"].fillna(\"\").str.lower() == \"open\"]\n",
    "    after_open = len(df)\n",
    "\n",
    "    df.attrs[\"cleaning_summary\"] = {\n",
    "        \"before\": before,\n",
    "        \"after_remove_zombie\": after_zombie,\n",
    "        \"after_keep_open\": after_open,\n",
    "        \"removed_zombie\": before - after_zombie,\n",
    "        \"removed_closed\": after_zombie - after_open\n",
    "    }\n",
    "    return df\n",
    "\n",
    "df_clean = clean_filter_only(df_raw)\n",
    "pd.DataFrame([df_clean.attrs[\"cleaning_summary\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951a6dd",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Derive proxy label + aggregate to ID-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa088cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean.copy()\n",
    "df[\"si_offering_row\"] = df[\"OFFERING_NAME\"].astype(str).str.contains(r\"\\bSI\\b\", case=False, na=False).astype(int)\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    s2 = s.dropna()\n",
    "    if len(s2) == 0:\n",
    "        return np.nan\n",
    "    m = s2.mode()\n",
    "    return m.iloc[0] if len(m) else s2.iloc[0]\n",
    "\n",
    "agg = {\n",
    "    \"OFFERING_NAME\": mode_or_first,\n",
    "    \"SI_CONSIDERATION_CD\": mode_or_first,\n",
    "    \"SFDR_PREF\": mode_or_first,\n",
    "    \"SFDR_ACTUAL\": mode_or_first,\n",
    "    \"PAI_PREF\": mode_or_first,\n",
    "    \"MIFID\": mode_or_first,\n",
    "    \"TAXONOMYPREF\": mode_or_first,\n",
    "    \"GHG\": mode_or_first,\n",
    "    \"Biodiversity\": mode_or_first,\n",
    "    \"Water\": mode_or_first,\n",
    "    \"Waste\": mode_or_first,\n",
    "    \"Social\": mode_or_first,\n",
    "    \"si_offering_row\": \"max\",\n",
    "}\n",
    "df_id = df.groupby(\"ID\", as_index=False).agg(agg).rename(columns={\"si_offering_row\":\"si_offering\"})\n",
    "df_id[\"si_offering\"] = df_id[\"si_offering\"].astype(int)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"level\":[\"row-level (cleaned)\",\"ID-level\"],\n",
    "    \"rows\":[len(df),len(df_id)],\n",
    "    \"si_offering_rate\":[df[\"si_offering_row\"].mean(), df_id[\"si_offering\"].mean()]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d77d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Encoding & improved SFDR/PAI signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2268c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SI = {\"S1\":1, \"S2\":2, \"S3\":3}\n",
    "MAP_SFDR = {\"F1\":1, \"F2\":2, \"F3\":3}\n",
    "MAP_TAX = {\"A1\":1, \"A2\":2, \"A3\":3}\n",
    "\n",
    "def parse_yes_no(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return 1 if x == 1 else 0\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        return 1 if x > 0.5 else 0\n",
    "    if isinstance(x, str):\n",
    "        t = x.strip().lower()\n",
    "        if t in {\"yes\",\"y\",\"true\",\"1\",\"selected\"}:\n",
    "            return 1\n",
    "        if t in {\"no\",\"n\",\"false\",\"0\"}:\n",
    "            return 0\n",
    "    return np.nan\n",
    "\n",
    "def parse_pai_selected(x):\n",
    "    # robust: catches \"PAI Selected\", \"pai selected\", \"PAI: Yes\", \"Yes\", \"Selected\", etc.\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return 0\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return 1 if x == 1 else 0\n",
    "    if isinstance(x, str):\n",
    "        t = x.strip().lower()\n",
    "        if \"pai\" in t and (\"select\" in t or \"yes\" in t or \"true\" in t or \"1\" == t):\n",
    "            return 1\n",
    "        if t in {\"pai selected\",\"selected\",\"yes\",\"true\",\"1\"}:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def parse_sfdr_level(x):\n",
    "    # robust: extracts F1/F2/F3 even if embedded\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        t = x.strip().upper()\n",
    "        for k in [\"F1\",\"F2\",\"F3\"]:\n",
    "            if k in t:\n",
    "                return MAP_SFDR[k]\n",
    "    return np.nan\n",
    "\n",
    "def encode(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Topics Yes/No\n",
    "    for c in [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]:\n",
    "        yn = df[c].apply(parse_yes_no)\n",
    "        df[c] = yn.fillna(0).astype(int)\n",
    "\n",
    "    # MIFID\n",
    "    df[\"MIFID\"] = df[\"MIFID\"].apply(parse_yes_no).fillna(0).astype(int)\n",
    "\n",
    "    # PAI\n",
    "    df[\"pai_selected\"] = df[\"PAI_PREF\"].apply(parse_pai_selected).astype(int)\n",
    "\n",
    "    # SI\n",
    "    df[\"SI_CONSIDERATION_num\"] = df[\"SI_CONSIDERATION_CD\"].map(MAP_SI)\n",
    "    df[\"si_missing\"] = df[\"SI_CONSIDERATION_num\"].isna().astype(int)\n",
    "    df[\"SI_CONSIDERATION_num\"] = df[\"SI_CONSIDERATION_num\"].fillna(1).astype(int)\n",
    "    df[\"si_norm\"] = np.clip((df[\"SI_CONSIDERATION_num\"] - 1)/2, 0, 1)\n",
    "\n",
    "    # SFDR\n",
    "    df[\"SFDR_PREF_num\"] = df[\"SFDR_PREF\"].apply(parse_sfdr_level)\n",
    "    df[\"SFDR_ACTUAL_num\"] = df[\"SFDR_ACTUAL\"].apply(parse_sfdr_level)\n",
    "    df[\"sfdr_pref_missing\"] = df[\"SFDR_PREF_num\"].isna().astype(int)\n",
    "    df[\"sfdr_actual_missing\"] = df[\"SFDR_ACTUAL_num\"].isna().astype(int)\n",
    "\n",
    "    # fill missing with 1 only for gap arithmetic, but keep missing flags so model can learn that missing != F1\n",
    "    pref_filled = df[\"SFDR_PREF_num\"].fillna(1)\n",
    "    actual_filled = df[\"SFDR_ACTUAL_num\"].fillna(1)\n",
    "\n",
    "    df[\"sfdr_pref_norm\"] = np.clip((pref_filled - 1)/2, 0, 1)        # preference level\n",
    "    df[\"sfdr_actual_norm\"] = np.clip((actual_filled - 1)/2, 0, 1)    # actual level (captures F3 as positive)\n",
    "\n",
    "    df[\"sfdr_gap\"] = np.clip(pref_filled - actual_filled, -2, 2)\n",
    "    df[\"sfdr_gap_signed_norm\"] = df[\"sfdr_gap\"] / 2.0                # -1..1\n",
    "    df[\"sfdr_opp_norm\"] = np.maximum(df[\"sfdr_gap\"], 0) / 2.0        # 0..1 upgrade opportunity\n",
    "\n",
    "    # Taxonomy\n",
    "    df[\"TAXONOMYPREF_num\"] = df[\"TAXONOMYPREF\"].map(MAP_TAX)\n",
    "    df[\"tax_missing\"] = df[\"TAXONOMYPREF_num\"].isna().astype(int)\n",
    "    df[\"TAXONOMYPREF_num\"] = df[\"TAXONOMYPREF_num\"].fillna(1).astype(int)\n",
    "    df[\"tax_norm\"] = np.clip((df[\"TAXONOMYPREF_num\"] - 1)/2, 0, 1)\n",
    "\n",
    "    # Topics + PAI block\n",
    "    topic_cols = [\"GHG\",\"Biodiversity\",\"Water\",\"Waste\",\"Social\"]\n",
    "    df[\"esg_topics_yes_cnt\"] = df[topic_cols].sum(axis=1)\n",
    "    df[\"topics_norm\"] = df[\"esg_topics_yes_cnt\"] / len(topic_cols)\n",
    "\n",
    "    # Keep same semantics: if no PAI, 0; if PAI selected, baseline + intensity\n",
    "    df[\"pai_block\"] = np.where(df[\"pai_selected\"]==1, 0.5 + 0.5*df[\"topics_norm\"], 0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = encode(df_id)\n",
    "\n",
    "# Diagnostics: are signals non-zero?\n",
    "diag = pd.DataFrame({\n",
    "    \"feature\": [\"si_norm\",\"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_block\",\"tax_norm\",\"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"],\n",
    "    \"mean\": [df_feat[c].mean() for c in [\"si_norm\",\"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_block\",\"tax_norm\",\"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"]],\n",
    "    \"nonzero_rate\": [(df_feat[c]!=0).mean() for c in [\"si_norm\",\"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_block\",\"tax_norm\",\"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"]]\n",
    "}).round(4)\n",
    "display(diag)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"sfdr_actual_norm\"], bins=10)\n",
    "plt.title(\"sfdr_actual_norm (captures F3 as 1.0)\")\n",
    "plt.xlabel(\"sfdr_actual_norm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df_feat[\"sfdr_opp_norm\"], bins=10)\n",
    "plt.title(\"sfdr_opp_norm (only positive preference>actual)\")\n",
    "plt.xlabel(\"sfdr_opp_norm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802669e",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_feat[\"si_offering\"].astype(int).copy()\n",
    "\n",
    "BASE_COLS = [\n",
    "    \"MIFID\",\"si_norm\",\n",
    "    \"sfdr_actual_norm\",\"sfdr_opp_norm\",\n",
    "    \"pai_block\",\"tax_norm\",\n",
    "    \"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"\n",
    "]\n",
    "X = df_feat[BASE_COLS].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "idx_train, idx_val = X_train.index, X_val.index\n",
    "\n",
    "print(\"Train:\", len(idx_train), \"Val:\", len(idx_val))\n",
    "print(\"Train si rate:\", y_train.mean().round(4), \"Val si rate:\", y_val.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f53a5",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ffe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global(y_true, p, label):\n",
    "    return {\n",
    "        \"model\": label,\n",
    "        \"auc\": roc_auc_score(y_true, p),\n",
    "        \"avg_precision\": average_precision_score(y_true, p),\n",
    "        \"brier\": brier_score_loss(y_true, p)\n",
    "    }\n",
    "\n",
    "def precision_lift_at_frac(y_true, p, frac=0.10):\n",
    "    n = len(p)\n",
    "    k = max(1, int(np.ceil(frac*n)))\n",
    "    order = np.argsort(-p)\n",
    "    top = y_true[order][:k]\n",
    "    baseline = y_true.mean()\n",
    "    prec = top.mean()\n",
    "    lift = (prec / baseline) if baseline > 0 else np.nan\n",
    "    return {\"frac\": frac, \"k\": k, \"precision\": float(prec), \"lift\": float(lift), \"baseline\": float(baseline)}\n",
    "\n",
    "def lift_by_decile(y_true, p, n_bins=10):\n",
    "    tmp = pd.DataFrame({\"y\": y_true, \"p\": p})\n",
    "    tmp[\"decile\"] = pd.qcut(tmp[\"p\"], n_bins, labels=False, duplicates=\"drop\") + 1\n",
    "    out = tmp.groupby(\"decile\")[\"y\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"si_rate\"})\n",
    "    return out\n",
    "\n",
    "def plot_lift_curve(tab, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(tab.index, tab[\"si_rate\"].values, marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Decile (1=lowest, 10=highest)\")\n",
    "    plt.ylabel(\"si_offering rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration(y_true, p, title):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed rate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90fe4c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) Fixed rule (alpha=0.80) — confirmations include SFDR_ACTUAL\n",
    "\n",
    "### Why include SFDR_ACTUAL\n",
    "`SFDR_ACTUAL=F3` is treated as **positive alignment** via `sfdr_actual_norm`. This avoids missing the signal when there is no upgrade gap.\n",
    "\n",
    "Fixed confirmation weights (sum to 1):\n",
    "- 50% SFDR actual alignment (`sfdr_actual_norm`)\n",
    "- 20% SFDR opportunity (`sfdr_opp_norm`)\n",
    "- 20% PAI (`pai_block`)\n",
    "- 10% Taxonomy (`tax_norm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d570b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FixedCfg:\n",
    "    alpha: float = 0.80\n",
    "    w_sfdr_actual: float = 0.50\n",
    "    w_sfdr_opp: float = 0.20\n",
    "    w_pai: float = 0.20\n",
    "    w_tax: float = 0.10\n",
    "\n",
    "cfg = FixedCfg()\n",
    "\n",
    "def score_fixed(df: pd.DataFrame, cfg: FixedCfg) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    confirm = (\n",
    "        cfg.w_sfdr_actual*df[\"sfdr_actual_norm\"] +\n",
    "        cfg.w_sfdr_opp*df[\"sfdr_opp_norm\"] +\n",
    "        cfg.w_pai*df[\"pai_block\"] +\n",
    "        cfg.w_tax*df[\"tax_norm\"]\n",
    "    )\n",
    "    m1 = cfg.alpha*df[\"si_norm\"] + (1-cfg.alpha)*confirm\n",
    "    m0 = df[\"si_norm\"]\n",
    "    df[\"p_fixed\"] = np.clip(np.where(df[\"MIFID\"]==1, m1, m0), 0, 1)\n",
    "\n",
    "    df[\"why_si\"] = np.where(df[\"MIFID\"]==1, cfg.alpha*df[\"si_norm\"], df[\"si_norm\"])\n",
    "    df[\"why_sfdr_actual\"] = np.where(df[\"MIFID\"]==1, (1-cfg.alpha)*cfg.w_sfdr_actual*df[\"sfdr_actual_norm\"], 0.0)\n",
    "    df[\"why_sfdr_opp\"] = np.where(df[\"MIFID\"]==1, (1-cfg.alpha)*cfg.w_sfdr_opp*df[\"sfdr_opp_norm\"], 0.0)\n",
    "    df[\"why_pai\"] = np.where(df[\"MIFID\"]==1, (1-cfg.alpha)*cfg.w_pai*df[\"pai_block\"], 0.0)\n",
    "    df[\"why_tax\"] = np.where(df[\"MIFID\"]==1, (1-cfg.alpha)*cfg.w_tax*df[\"tax_norm\"], 0.0)\n",
    "    return df\n",
    "\n",
    "df_scored = score_fixed(df_feat, cfg)\n",
    "\n",
    "p_fixed = df_scored.loc[idx_val, \"p_fixed\"].values\n",
    "fixed_global = eval_global(y_val.values, p_fixed, \"Fixed rule (sfdr_actual+opp)\")\n",
    "fixed_top10 = precision_lift_at_frac(y_val.values, p_fixed, 0.10)\n",
    "fixed_top20 = precision_lift_at_frac(y_val.values, p_fixed, 0.20)\n",
    "\n",
    "display(pd.DataFrame([fixed_global]))\n",
    "display(pd.DataFrame([fixed_top10, fixed_top20]))\n",
    "\n",
    "tab = lift_by_decile(y_val.values, p_fixed)\n",
    "display(tab)\n",
    "plot_lift_curve(tab, \"Lift by decile: Fixed rule (sfdr_actual+opp)\")\n",
    "plot_calibration(y_val.values, p_fixed, \"Calibration: Fixed rule (sfdr_actual+opp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924b7a1",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Weighted rule (learn weights + alpha) — keeps SFDR_ACTUAL signal\n",
    "\n",
    "We learn confirmation weights from training data using logistic regression, then:\n",
    "- keep only **non-negative** contributions (stakeholder-friendly “supportive evidence”)\n",
    "- normalize to sum to 1\n",
    "\n",
    "We also learn alpha within 0.60–0.90 by CV to improve top-bucket performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c122dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_scored.loc[idx_train].copy()\n",
    "train_m1 = train_df[train_df[\"MIFID\"]==1].copy()\n",
    "\n",
    "CONF = [\"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_block\",\"tax_norm\",\"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"]\n",
    "\n",
    "# Fit on MIFID=1 subset (so confirmations are relevant)\n",
    "if train_m1[\"si_offering\"].nunique() < 2:\n",
    "    print(\"Warning: MIFID=1 train subset has one class; fallback to fixed weights.\")\n",
    "    w = pd.Series([cfg.w_sfdr_actual, cfg.w_sfdr_opp, cfg.w_pai, cfg.w_tax, 0.0, 0.0, 0.0], index=CONF)\n",
    "else:\n",
    "    lr = LogisticRegression(max_iter=8000, class_weight=\"balanced\")\n",
    "    lr.fit(train_m1[CONF], train_m1[\"si_offering\"])\n",
    "    coef = pd.Series(lr.coef_[0], index=CONF).sort_values(key=np.abs, ascending=False)\n",
    "    display(coef.to_frame(\"raw_coef (train, MIFID=1)\"))\n",
    "\n",
    "    # keep only positive as \"supportive evidence\"\n",
    "    pos = np.maximum(lr.coef_[0], 0)\n",
    "    if pos.sum() == 0:\n",
    "        pos = np.ones_like(pos)\n",
    "    w = pd.Series(pos/pos.sum(), index=CONF)\n",
    "\n",
    "display(w.to_frame(\"learned_weight (sum=1)\"))\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.bar(w.index, w.values)\n",
    "plt.title(\"Learned confirmation weights (sum=1)\")\n",
    "plt.ylabel(\"weight\")\n",
    "plt.xticks(rotation=35, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def score_prob(df: pd.DataFrame, alpha: float, w: pd.Series) -> np.ndarray:\n",
    "    confirm = sum(w[c]*df[c] for c in w.index)\n",
    "    m1 = alpha*df[\"si_norm\"].values + (1-alpha)*confirm.values\n",
    "    m0 = df[\"si_norm\"].values\n",
    "    return np.clip(np.where(df[\"MIFID\"].values==1, m1, m0), 0, 1)\n",
    "\n",
    "# Choose alpha by CV AP on train\n",
    "alpha_grid = np.round(np.arange(0.60, 0.91, 0.05), 2)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "train_idx = idx_train.values\n",
    "rows=[]\n",
    "for a in alpha_grid:\n",
    "    aps=[]\n",
    "    for tr_i, te_i in skf.split(train_idx, y_train.values):\n",
    "        te_ix = train_idx[te_i]\n",
    "        df_te = df_scored.loc[te_ix]\n",
    "        p = score_prob(df_te, a, w)\n",
    "        aps.append(average_precision_score(df_te[\"si_offering\"].values, p))\n",
    "    rows.append({\"alpha\": a, \"cv_ap_mean\": float(np.mean(aps))})\n",
    "\n",
    "alpha_perf = pd.DataFrame(rows).sort_values(\"cv_ap_mean\", ascending=False)\n",
    "display(alpha_perf)\n",
    "\n",
    "alpha_best = float(alpha_perf.iloc[0][\"alpha\"])\n",
    "print(\"Selected alpha:\", alpha_best)\n",
    "\n",
    "df_scored[\"p_weighted\"] = score_prob(df_scored, alpha_best, w)\n",
    "\n",
    "p_w = df_scored.loc[idx_val, \"p_weighted\"].values\n",
    "w_global = eval_global(y_val.values, p_w, f\"Weighted rule (alpha={alpha_best})\")\n",
    "w_top10 = precision_lift_at_frac(y_val.values, p_w, 0.10)\n",
    "w_top20 = precision_lift_at_frac(y_val.values, p_w, 0.20)\n",
    "\n",
    "display(pd.DataFrame([w_global]))\n",
    "display(pd.DataFrame([w_top10, w_top20]))\n",
    "\n",
    "tab = lift_by_decile(y_val.values, p_w)\n",
    "display(tab)\n",
    "plot_lift_curve(tab, \"Lift by decile: Weighted rule (sfdr_actual+opp)\")\n",
    "plot_calibration(y_val.values, p_w, \"Calibration: Weighted rule (sfdr_actual+opp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f4679",
   "metadata": {},
   "source": [
    "---\n",
    "## 9) ML challenger — Calibrated Logistic Regression\n",
    "\n",
    "We include MiFID interactions so confirmation signals contribute only when `MIFID=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dac903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X[\"si_norm\"] = df[\"si_norm\"]\n",
    "    X[\"MIFID\"] = df[\"MIFID\"]\n",
    "    # MiFID interactions\n",
    "    for c in [\"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_block\",\"tax_norm\",\"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"]:\n",
    "        X[f\"m1_{c}\"] = df[\"MIFID\"] * df[c]\n",
    "    return X\n",
    "\n",
    "Xtr = ml_matrix(df_scored.loc[idx_train])\n",
    "Xva = ml_matrix(df_scored.loc[idx_val])\n",
    "\n",
    "base_lr = LogisticRegression(max_iter=12000, class_weight=\"balanced\")\n",
    "cal = CalibratedClassifierCV(base_lr, method=\"isotonic\", cv=5)\n",
    "cal.fit(Xtr, y_train)\n",
    "\n",
    "p_ml = cal.predict_proba(Xva)[:,1]\n",
    "ml_global = eval_global(y_val.values, p_ml, \"ML: Calibrated LR (sfdr_actual+opp)\")\n",
    "ml_top10 = precision_lift_at_frac(y_val.values, p_ml, 0.10)\n",
    "ml_top20 = precision_lift_at_frac(y_val.values, p_ml, 0.20)\n",
    "\n",
    "display(pd.DataFrame([ml_global]))\n",
    "display(pd.DataFrame([ml_top10, ml_top20]))\n",
    "\n",
    "tab = lift_by_decile(y_val.values, p_ml)\n",
    "display(tab)\n",
    "plot_lift_curve(tab, \"Lift by decile: ML (sfdr_actual+opp)\")\n",
    "plot_calibration(y_val.values, p_ml, \"Calibration: ML (sfdr_actual+opp)\")\n",
    "\n",
    "# interpret coefficients\n",
    "plain = LogisticRegression(max_iter=12000, class_weight=\"balanced\")\n",
    "plain.fit(Xtr, y_train)\n",
    "coef = pd.Series(plain.coef_[0], index=Xtr.columns).sort_values(key=np.abs, ascending=False)\n",
    "display(coef.to_frame(\"coef\"))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(coef.index[:12], coef.values[:12])\n",
    "plt.title(\"Top coefficients (uncalibrated LR; direction sanity-check)\")\n",
    "plt.ylabel(\"coef\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291d4b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10) Compare validation performance + operational output\n",
    "\n",
    "We pick a champion based on top-bucket lift/precision and calibration stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([fixed_global, w_global, ml_global]).round(4)\n",
    "display(comparison)\n",
    "\n",
    "topk = pd.DataFrame([\n",
    "    {\"model\":\"Fixed\", **fixed_top10},\n",
    "    {\"model\":\"Fixed\", **fixed_top20},\n",
    "    {\"model\":\"Weighted\", **w_top10},\n",
    "    {\"model\":\"Weighted\", **w_top20},\n",
    "    {\"model\":\"ML\", **ml_top10},\n",
    "    {\"model\":\"ML\", **ml_top20},\n",
    "]).round(4)\n",
    "display(topk)\n",
    "\n",
    "# Operational output (use weighted by default)\n",
    "df_out = df_scored.copy()\n",
    "df_out[\"rank_prob\"] = df_out[\"p_weighted\"]\n",
    "df_out[\"pct\"] = (df_out[\"rank_prob\"].rank(pct=True)*100).round(2)\n",
    "df_out[\"bucket_3\"] = pd.cut(df_out[\"pct\"], bins=[-0.01,50,80,100], labels=[\"Low\",\"Average\",\"High\"])\n",
    "\n",
    "targets = df_out[df_out[\"si_offering\"]==0].sort_values(\"rank_prob\", ascending=False).head(20)\n",
    "cols = [\n",
    "    \"ID\",\"rank_prob\",\"pct\",\"bucket_3\",\n",
    "    \"MIFID\",\"SI_CONSIDERATION_num\",\"SFDR_ACTUAL_num\",\"SFDR_PREF_num\",\"sfdr_gap\",\n",
    "    \"sfdr_actual_norm\",\"sfdr_opp_norm\",\"pai_selected\",\"pai_block\",\"TAXONOMYPREF_num\",\"tax_norm\",\n",
    "    \"sfdr_pref_missing\",\"sfdr_actual_missing\",\"tax_missing\"\n",
    "]\n",
    "targets[cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
